{
    "name": "jasmine",
    "description": "SOURCE: [ChaLearn Automatic Machine Learning Challenge (AutoML)](https://competitions.codalab.org/competitions/2321), [ChaLearn](https://automl.chalearn.org/data) \n\nThis is a \"supervised learning\" challenge in machine learning. We are making available 30 datasets, all pre-formatted in given feature representations (this means that each example consists of a fixed number of numerical coefficients). The challenge is to solve classification and regression problems, without any further human intervention.\n\nThe difficulty is that there is a broad diversity of data types and distributions (including balanced or unbalanced classes, sparse or dense feature representations, with or without missing values or categorical variables, various metrics of evaluation, various proportions of number of features and number of examples). The problems are drawn from a wide variety of domains and include medical diagnosis from laboratory analyses, speech recognition, credit rating, prediction or drug toxicity or efficacy, classification of text, prediction of customer satisfaction, object recognition, protein structure prediction, action recognition in video data, etc. While there exist machine learning toolkits including methods that can solve all these problems, it is still considerable human effort to find, for a given combination of dataset, task, metric of evaluation, and available computational time, the combination of methods and hyper-parameter setting that is best suited. Your challenge is to create the \"perfect black box\" eliminating the human in the loop.\n\nThis is a challenge with code submission: your code will be executed automatically on our servers to train and test your learning machines with unknown datasets. However, there is NO OBLIGATION TO SUBMIT CODE. Half of the prizes can be won by just submitting prediction results. There are six rounds (Prep, Novice, Intermediate, Advanced, Expert, and Master) in which datasets of progressive difficulty are introduced (5 per round). There is NO PREREQUISITE TO PARTICIPATE IN PREVIOUS ROUNDS to enter a new round. The rounds alternate AutoML phases in which submitted code is \"blind tested\" in limited time on our platform, using datasets you have never seen before, and Tweakathon phases giving you time to improve your methods by tweaking them on those datasets and running them on your own systems (without computational resource limitation).\n\nNOTE: This dataset corresponds to one of the datasets of the challenge.",
    "target_col": "class",
    "metadata": {
        "NumberOfClasses": 2.0,
        "NumberOfFeatures": 145.0,
        "NumberOfInstances": 2984.0,
        "NumberOfInstancesWithMissingValues": 0.0,
        "NumberOfMissingValues": 0.0,
        "NumberOfNumericFeatures": 8.0,
        "NumberOfSymbolicFeatures": 137.0,
        "AutoCorrelation": 0.5062018102581294,
        "ClassEntropy": 1.0,
        "Dimensionality": 0.04859249329758713,
        "EquivalentNumberOfAtts": 41.90776022599679,
        "MajorityClassPercentage": 50.0,
        "MajorityClassSize": 1492.0,
        "MaxAttributeEntropy": 0.9994552098049188,
        "MaxKurtosisOfNumericAtts": 108.46584697335489,
        "MaxMeansOfNumericAtts": 2.1702412868632703,
        "MaxMutualInformation": 0.23272096971787,
        "MaxNominalAttDistinctValues": 2.0,
        "MaxSkewnessOfNumericAtts": 9.959126954446072,
        "MaxStdDevOfNumericAtts": 2.1759294411147314,
        "MeanAttributeEntropy": 0.5169986497758877,
        "MeanKurtosisOfNumericAtts": 58.06816815679465,
        "MeanMeansOfNumericAtts": 0.20351911234081763,
        "MeanMutualInformation": 0.023861929022388234,
        "MeanNoiseToSignalRatio": 20.66625545197199,
        "MeanNominalAttDistinctValues": 2.0,
        "MeanSkewnessOfNumericAtts": 6.272359761925547,
        "MeanStdDevOfNumericAtts": 0.9416444717861043,
        "MinAttributeEntropy": 0.004351704120539798,
        "MinKurtosisOfNumericAtts": 1.3662909971010686,
        "MinMeansOfNumericAtts": -0.5613814778820376,
        "MinMutualInformation": 0.0,
        "MinNominalAttDistinctValues": 2.0,
        "MinSkewnessOfNumericAtts": 0.9710213503265842,
        "MinStdDevOfNumericAtts": 0.3760150295720551,
        "MinorityClassPercentage": 50.0,
        "MinorityClassSize": 1492.0,
        "NumberOfBinaryFeatures": 137.0,
        "PercentageOfBinaryFeatures": 94.48275862068965,
        "PercentageOfInstancesWithMissingValues": 0.0,
        "PercentageOfMissingValues": 0.0,
        "PercentageOfNumericFeatures": 5.517241379310345,
        "PercentageOfSymbolicFeatures": 94.48275862068965,
        "Quartile1AttributeEntropy": 0.2602717929793128,
        "Quartile1KurtosisOfNumericAtts": 3.5707061817498014,
        "Quartile1MeansOfNumericAtts": -0.5591782827161529,
        "Quartile1MutualInformation": 0.000123923251345,
        "Quartile1SkewnessOfNumericAtts": 1.2407279155447668,
        "Quartile1StdDevOfNumericAtts": 0.6328909848508008,
        "Quartile2AttributeEntropy": 0.44913483212318284,
        "Quartile2KurtosisOfNumericAtts": 76.80772339094872,
        "Quartile2MeansOfNumericAtts": -0.5486132259215818,
        "Quartile2MutualInformation": 0.000983712208565,
        "Quartile2SkewnessOfNumericAtts": 8.65723099306124,
        "Quartile2StdDevOfNumericAtts": 0.8472091609471499,
        "Quartile3AttributeEntropy": 0.8571314997138089,
        "Quartile3KurtosisOfNumericAtts": 98.25725167836738,
        "Quartile3MeansOfNumericAtts": 1.1528548298424934,
        "Quartile3MutualInformation": 0.0202036782210775,
        "Quartile3SkewnessOfNumericAtts": 9.681105581898798,
        "Quartile3StdDevOfNumericAtts": 0.9853706765834508,
        "StdvNominalAttDistinctValues": 0.0
    },
    "df_head": " class  V1  V2  V3  V4  V5  V6  V7  V8  V9  V10  V11  V12       V13  V14  V15  V16  V17  V18  V19  V20  V21  V22       V23  V24  V25  V26  V27  V28  V29  V30  V31  V32  V33  V34  V35  V36  V37  V38  V39  V40  V41  V42      V43  V44  V45  V46  V47  V48  V49  V50  V51  V52  V53  V54  V55      V56  V57  V58       V59  V60  V61  V62  V63  V64  V65  V66  V67  V68  V69  V70  V71  V72  V73  V74  V75  V76  V77  V78  V79  V80  V81  V82  V83  V84  V85  V86  V87  V88  V89  V90  V91  V92  V93  V94  V95  V96  V97  V98  V99  V100  V101  V102  V103  V104  V105  V106  V107  V108  V109  V110  V111  V112  V113  V114  V115  V116  V117  V118  V119  V120  V121  V122  V123  V124  V125      V126  V127  V128  V129  V130      V131  V132  V133  V134  V135  V136  V137  V138  V139  V140  V141  V142  V143  V144\n     1   1   0   0   0   0   1   1   0   1    0    0    1 -0.555556    0    0    0    0    0    1    1    1    1 -0.478261    0    0    1    1    0    0    0    0    0    1    1    0    0    0    0    1    1    0    0 1.571430    0    5    0    1    0    1    0    0    0    1    1    1 0.890909    1    0 -0.781818    1    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    0    1    0    1    0    1    0    1    0    0    0    0    0    0    1    0    0    0    0    1    1     1     0     0     0     1     1     0     0     1     0     0     0     1     0     0     0     1     1     0     0     0     1     0     0     0     0 -0.733333     0     0     0     0 -0.647059     0     0     1     1     0     1     0     1     0     1     0     0     0\n     1   0   0   0   0   0   0   0   0   0    0    0    0 -0.625000    0    0    0    0    1    0    0    1    1 -0.851852    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    1    1    0    0 1.028570    0    3    0    0    0    0    0    0    0    1    0    0 1.416670    1    0 -0.666667    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    0    0    1    1    1    0    0    0    0    0    0    0    0    0    0    1    0    0    0    0    1    0    1     0     0     1     0     0     1     0     0     1     1     0     1     1     1     0     0     0     0     0     0     1     0     0     0     0     0 -0.520000     0     0     0     0 -0.612903     1     0     1     0     0     0     1     0     1     0     0     0     0\n     0   0   0   0   0   0   1   0   0   0    0    0    0 -0.636364    0    0    0    0    1    0    0    0    0 -0.684211    0    0    1    1    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0 0.942857    0    0    0    0    0    0    1    0    0    0    0    0 0.212121    0    0 -0.636364    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    1    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    1    0    0     1     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0     0 -0.555556     0     0     0     0 -0.714286     0     0     0     0     0     1     0     0     1     0     0     0     1\n     0   0   0   0   0   0   1   0   0   1    0    0    0 -0.612903    0    0    0    0    0    0    0    0    0 -0.733333    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0 0.800000    1    0    0    0    0    0    0    0    0    0    0    0 0.250000    0    0 -0.571429    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    0    1    0    1    0    1    0    0     1     0     0     0     0     1     0     0     0     0     0     0     0     0     0     0     0     0     1     0     0     0     0     0     0     0 -0.789474     0     0     0     0 -0.200000     0     0     0     0     0     0     0     0     0     0     1     0     0\n     1   0   0   0   0   0   0   1   0   1    0    1    0 -0.733333    0    0    0    1    0    0    0    1    1 -0.538462    0    0    1    1    0    0    0    0    0    0    0    1    0    1    1    1    1    1    0 1.628570    0    2    0    0    0    0    0    0    0    0    0    0 0.754386    0    0 -0.789474    0    0    0    1    0    0    0    1    0    0    0    0    0    0    1    0    1    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0    1    0    0    0    0    0    0    0     0     0     1     1     0     0     0     0     0     1     0     0     1     0     0     0     0     0     0     0     0     0     0     0     0     0 -0.625000     0     0     0     0 -0.538462     1     0     1     0     0     0     1     0     0     0     1     0     0"
}