### UNMODIFIABLE IMPORT BEGIN ###
import random
from pathlib import Path
import numpy as np
from typing import Tuple
from fedot.api.main import Fedot
from fedot.core.pipelines.pipeline import Pipeline
from fedot.core.data.data import InputData
from fedot.core.repository.tasks import Task
from fedot.core.repository.tasks import TaskTypesEnum # classification, regression, ts_forecasting.
from automl import train_model, evaluate_model
### UNMODIFIABLE IMPORT END ###
# USER CODE BEGIN IMPORTS #
from sklearn.model_selection import train_test_split
import pandas as pd
# USER CODE END IMPORTS #

SEED = 42
random.seed(SEED)
np.random.seed(SEED)

### UNMODIFIABLE CODE BEGIN ###
DATASET_PATH = Path("{%dataset_path%}") # path for saving and loading dataset(s)
PIPELINE_PATH = Path("{%work_dir_path%}") / "pipeline" # path for saving and loading pipelines
### UNMODIFIABLE CODE END ###

# USER CODE BEGIN LOAD_DATA #
def load_data():
    # TODO: this function is for loading a dataset from user’s local storage
    return train, X_test
# USER CODE END LOAD_DATA #

def transform_data(data: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:
    # TODO: This function is for transforming the data into a format that can be used for training the model.
    # TODO: You are using this function to process both Train and Test data. Test data may initially not contain target columns.
    # TODO: Before any operations, make sure to check whether columns you operate on are present in data. Do not raise exceptions.
    # TODO: Only dropping columns is allowed, all other dataset transformations are STRICTLY FORBIDDEN. 
    # TODO: Impute Missing Value in Test Data. 
    # TODO: If numeric columns are present, use imputer 'mean' strategy ONLY on them, 'most_frequent' on other coumns if there are any.
    return features, target


# The main function to orchestrate the data loading, feature engineering, model training and model evaluation
def create_model():
    """
    Function to execute the ML pipeline.
    """
    # USER CODE BEGIN CREATE MODEL #
    # TODO: Step 1. Retrieve or load a dataset from hub (if available) or user’s local storage, start path from the DATASET_PATH
    train, X_test = load_data()
    
    # TODO: Step 2. Create a train-test split of the data by splitting the ‘dataset‘ into train_data and test_data.
    # Here, the train_data contains 80% of the ‘dataset‘ and the test_data contains 20% of the ‘dataset‘.
    train_data, eval_test_data = train_test_split(train, test_size=0.2, random_state=SEED) # corresponding to 80%, 20% of ‘dataset‘
    
    train_features, train_target = transform_data(train_data)
    eval_test_features, eval_test_target = transform_data(eval_test_data)
    test_features, _ = transform_data(X_test)
    
    
    # TODO: Step 3. Train AutoML model. AutoML performs feature engineering and model training.
    model = train_model(train_features: np.ndarray, train_target: np.ndarray)
    
    # TODO: Step 4. evaluate the trained model using the defined "evaluate_model" function model_performance, model_complexity = evaluate_model()
    model_performance = evaluate_model(model, eval_test_features: np.ndarray, eval_test_target: np.ndarray)

    # Evaluate predictions for the test datase using AutoML Framework
    # **YOU MUST USE automl_predict()**
    # Prediction result will not have an ID column, only a column for target (or columns for multiple targets)
    # If output sample submission has an ID column, add it to the prediction
    predictions:np.ndarray = automl_predict(model, test_features: np.ndarray) # returns 2D array
    output = pd.DataFrame(predictions, columns=[...])
    output.to_csv(Path({%work_dir_path%}) / 'submission.csv', index=False)
    # USER CODE END CREATE MODEL # 
    return model_performance


### UNMODIFIABLE CODE BEGIN ###
def main():
    """ 
    Main function to execute the ML pipeline.
    """
    model_performance = create_model()
    print("Model Performance on Test Set:", model_performance)
        
if __name__ == "__main__":
    main()
### UNMODIFIABLE CODE END ###z