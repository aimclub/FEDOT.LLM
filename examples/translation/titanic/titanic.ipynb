{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Survival Prediction using FEDOT and LLM\n",
    "\n",
    "This notebook demonstrates the process of analyzing the Titanic dataset and predicting passenger survival using the FEDOT framework enhanced with Large Language Models (LLM).\n",
    "\n",
    "https://www.kaggle.com/competitions/titanic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/hyperopt/atpe.py:19: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  import pkg_resources\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading dataset to: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition\n",
      "Dataset downloaded and extracted to /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "\n",
    "from fedotllm.main import FedotAI\n",
    "from fedotllm.output import JupyterOutput\n",
    "from fedotllm.llm import AIInference\n",
    "from examples.kaggle import download_from_kaggle, submit_to_kaggle\n",
    "\n",
    "competition_name = \"titanic\"\n",
    "dataset_path = os.path.join(os.getcwd(), \"competition\")\n",
    "download_from_kaggle(competition_name=competition_name, save_path=dataset_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "description = \"\"\"\n",
    "Построй модель AutoML. Задача «Титаник - машинное обучение на основе катастрофы» на Kaggle заключается в предсказании того, выжил ли пассажир после затопления «Титаника», используя предоставленные наборы данных. Набор данных train.csv включает 891 пассажира с их статусом выживания (ground truth), а test.csv содержит аналогичную информацию, но без статуса выживания. Участники строят модель машинного обучения, используя закономерности в обучающих данных, чтобы предсказать результаты выживания в тестовых данных. Материалы должны быть представлены в формате CSV, содержащем ровно 418 строк и два столбца: PassengerId и Survived (бинарные прогнозы: 1 - выжил, 0 - умер). Метрикой оценки является точность, которая рассчитывается как процент правильных предсказаний.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,682 - FEDOTLLM - INFO - FedotAI ainvoke called. Input message (first 100 chars): '\n",
      "Построй модель AutoML. Задача «Титаник - машинное обучение на основе катастрофы» на Kaggle заключае...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,682 - FedotAI ainvoke called. Input message (first 100 chars): '\n",
      "Построй модель AutoML. Задача «Титаник - машинное обучение на основе катастрофы» на Kaggle заключае...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,688 - FEDOTLLM - INFO - TranslatorAgent initialized with provided AIInference instance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,688 - TranslatorAgent initialized with provided AIInference instance.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,689 - FEDOTLLM - INFO - FedotAI ainvoke: Translating input message to English.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,689 - FedotAI ainvoke: Translating input message to English.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,689 - FEDOTLLM - INFO - TranslatorAgent: Received input message for translation to English (first 200 chars): '\n",
      "Построй модель AutoML. Задача «Титаник - машинное обучение на основе катастрофы» на Kaggle заключается в предсказании того, выжил ли пассажир после затопления «Титаника», используя предоставленные на...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,689 - TranslatorAgent: Received input message for translation to English (first 200 chars): '\n",
      "Построй модель AutoML. Задача «Титаник - машинное обучение на основе катастрофы» на Kaggle заключается в предсказании того, выжил ли пассажир после затопления «Титаника», используя предоставленные на...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,824 - FEDOTLLM - INFO - TranslatorAgent: Source language for input set to: ru\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,824 - TranslatorAgent: Source language for input set to: ru\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,825 - FEDOTLLM - INFO - Translating input from ru to English using self.inference.query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,825 - Translating input from ru to English using self.inference.query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,826 - FEDOTLLM - INFO - TranslatorAgent._translate_text: Attempting translation from 'ru' to 'en' using self.inference.query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:40,826 - TranslatorAgent._translate_text: Attempting translation from 'ru' to 'en' using self.inference.query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:51,755 - FEDOTLLM - INFO - Successfully translated text from ru to en using self.inference.query.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:51,755 - Successfully translated text from ru to en using self.inference.query.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:51,759 - FEDOTLLM - INFO - FedotAI ainvoke: Input message translated to (first 100 chars): 'Build an AutoML model. The 'Titanic - Machine Learning from Disaster' challenge on Kaggle involves p...'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:51,759 - FedotAI ainvoke: Input message translated to (first 100 chars): 'Build an AutoML model. The 'Titanic - Machine Learning from Disaster' challenge on Kaggle involves p...'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:52,904 - FEDOTLLM - INFO - Running problem reflection\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:33:52,904 - Running problem reflection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:34:11,764 - FEDOTLLM - INFO - Running generate automl config\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:34:11,764 - Running generate automl config\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:34:17,993 - FEDOTLLM - INFO - Running select skeleton\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:34:17,993 - Running select skeleton\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:34:17,998 - FEDOTLLM - INFO - Generating code\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:34:17,998 - Generating code\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:16,712 - FEDOTLLM - INFO - Running insert templates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:16,712 - Running insert templates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:16,743 - FEDOTLLM - INFO - Running evaluate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:16,743 - Running evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:19,754 - FEDOTLLM - ERROR - Traceback (most recent call last):\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 211, in <module>\n",
      "    main()\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 207, in main\n",
      "    model_performance = create_model()\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 145, in create_model\n",
      "    train_features, train_target = transform_data(train_data)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 117, in transform_data\n",
      "    features[col] = categorical_imputer.fit_transform(features[[col]])\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 4311, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 4524, in _set_item\n",
      "    value, refs = self._sanitize_column(value)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 5267, in _sanitize_column\n",
      "    arr = sanitize_array(value, self.index, copy=True, allow_2d=True)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py\", line 1181, in maybe_infer_to_datetimelike\n",
      "    raise ValueError(value.ndim)  # pragma: no cover\n",
      "ValueError: 2\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:19,754 - Traceback (most recent call last):\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 211, in <module>\n",
      "    main()\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 207, in main\n",
      "    model_performance = create_model()\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 145, in create_model\n",
      "    train_features, train_target = transform_data(train_data)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/solution.py\", line 117, in transform_data\n",
      "    features[col] = categorical_imputer.fit_transform(features[[col]])\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 4311, in __setitem__\n",
      "    self._set_item(key, value)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 4524, in _set_item\n",
      "    value, refs = self._sanitize_column(value)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/frame.py\", line 5267, in _sanitize_column\n",
      "    arr = sanitize_array(value, self.index, copy=True, allow_2d=True)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/construction.py\", line 606, in sanitize_array\n",
      "    subarr = maybe_infer_to_datetimelike(data)\n",
      "  File \"/Users/aleksejlapin/Work/STABLE-FedotLLM/.venv/lib/python3.10/site-packages/pandas/core/dtypes/cast.py\", line 1181, in maybe_infer_to_datetimelike\n",
      "    raise ValueError(value.ndim)  # pragma: no cover\n",
      "ValueError: 2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:19,758 - FEDOTLLM - INFO - Running fix solution\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:19,758 - Running fix solution\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:50,584 - FEDOTLLM - INFO - Running insert templates\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:50,584 - Running insert templates\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:50,609 - FEDOTLLM - INFO - Running evaluate\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:35:50,609 - Running evaluate\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:36:54,512 - FEDOTLLM - INFO - Running tests\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:36:54,512 - Running tests\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:36:54,513 - FEDOTLLM - INFO - Test passed: {'accuracy': 0.788}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:36:54,513 - Test passed: {'accuracy': 0.788}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:36:55,123 - FEDOTLLM - INFO - Test passed: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "2025-06-11 16:36:55,123 - FEDOTLLM - INFO - Test passed: Submission file exists.\n",
      "2025-06-11 16:36:55,125 - FEDOTLLM - ERROR - Test failed: Error validating submission format: [Errno 2] No such file or directory: '/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/sample_submission.csv'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: RAW OUTPUT\n",
      " Files and directories:\n",
      "Dataset Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition\n",
      "Workspace Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output\n",
      "Pipeline Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/pipeline\n",
      "Submission Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/submission.csv\n",
      "Train File: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/train.csv\n",
      "Test File: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/test.csv\n",
      "Sample Submission File: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/sample_submission.csv\n",
      "2025-06-11 16:35:54,108 - ApiComposer - Initial pipeline was fitted in 0.7 sec.\n",
      "2025-06-11 16:35:54,108 - ApiComposer - Taking into account n_folds=5, estimated fit time for initial assumption is 3.5 sec.\n",
      "2025-06-11 16:35:54,111 - ApiComposer - AutoML configured. Parameters tuning: True. Time limit: 1.0 min. Set of candidate models: ['bernb', 'catboost', 'dt', 'fast_ica', 'isolation_forest_class', 'knn', 'lgbm', 'logit', 'mlp', 'normalization', 'pca', 'poly_features', 'qda', 'resample', 'rf', 'scaling', 'xgboost'].\n",
      "2025-06-11 16:35:54,167 - ApiComposer - Timeout is too small for composing and is skipped because fit_time is 3.47552 sec.\n",
      "2025-06-11 16:35:54,173 - ApiComposer - Hyperparameters tuning started with 1 min. timeout\n",
      "2025-06-11 16:35:56,629 - SimultaneousTuner - Initial graph: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1} \n",
      "Initial metric: [0.813]\n",
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?trial/s, best loss=?]\n",
      "  0%|          | 2/100000 [00:00<4:07:47,  6.73trial/s, best loss: inf]\n",
      "  0%|          | 4/100000 [00:00<3:10:17,  8.76trial/s, best loss: inf]\n",
      "  0%|          | 6/100000 [00:00<2:48:21,  9.90trial/s, best loss: inf]\n",
      "  0%|          | 8/100000 [00:00<2:39:41, 10.44trial/s, best loss: inf]\n",
      "  0%|          | 10/100000 [00:00<2:33:39, 10.85trial/s, best loss: inf]\n",
      "  0%|          | 12/100000 [00:01<2:30:19, 11.09trial/s, best loss: inf]\n",
      "  0%|          | 14/100000 [00:01<2:48:12,  9.91trial/s, best loss: inf]\n",
      "  0%|          | 16/100000 [00:01<2:44:27, 10.13trial/s, best loss: inf]\n",
      "  0%|          | 18/100000 [00:01<2:39:34, 10.44trial/s, best loss: inf]\n",
      "  0%|          | 20/100000 [00:01<2:38:29, 10.51trial/s, best loss: inf]\n",
      "  0%|          | 22/100000 [00:02<2:57:53,  9.37trial/s, best loss: inf]\n",
      "  0%|          | 23/100000 [00:02<3:05:47,  8.97trial/s, best loss: inf]\n",
      "  0%|          | 24/100000 [00:02<3:31:45,  7.87trial/s, best loss: inf]\n",
      "  0%|          | 25/100000 [00:02<3:34:36,  7.76trial/s, best loss: inf]\n",
      "  0%|          | 26/100000 [00:02<3:36:54,  7.68trial/s, best loss: inf]\n",
      "  0%|          | 27/100000 [00:02<3:39:36,  7.59trial/s, best loss: inf]\n",
      "  0%|          | 28/100000 [00:03<3:41:11,  7.53trial/s, best loss: inf]\n",
      "  0%|          | 29/100000 [00:03<4:13:42,  6.57trial/s, best loss: inf]\n",
      "  0%|          | 30/100000 [00:03<4:24:33,  6.30trial/s, best loss: inf]\n",
      "  0%|          | 31/100000 [00:03<4:24:19,  6.30trial/s, best loss: inf]\n",
      "  0%|          | 32/100000 [00:03<4:11:24,  6.63trial/s, best loss: inf]\n",
      "  0%|          | 33/100000 [00:03<4:04:17,  6.82trial/s, best loss: inf]\n",
      "  0%|          | 34/100000 [00:04<4:25:11,  6.28trial/s, best loss: inf]\n",
      "  0%|          | 35/100000 [00:04<4:16:02,  6.51trial/s, best loss: inf]\n",
      "  0%|          | 36/100000 [00:04<4:07:57,  6.72trial/s, best loss: inf]\n",
      "  0%|          | 37/100000 [00:04<4:01:56,  6.89trial/s, best loss: inf]\n",
      "  0%|          | 38/100000 [00:04<3:56:19,  7.05trial/s, best loss: inf]\n",
      "  0%|          | 39/100000 [00:04<3:54:25,  7.11trial/s, best loss: inf]\n",
      "  0%|          | 40/100000 [00:04<3:52:32,  7.16trial/s, best loss: inf]\n",
      "  0%|          | 41/100000 [00:05<3:54:39,  7.10trial/s, best loss: inf]\n",
      "  0%|          | 42/100000 [00:05<4:13:58,  6.56trial/s, best loss: inf]\n",
      "  0%|          | 43/100000 [00:05<4:07:18,  6.74trial/s, best loss: inf]\n",
      "  0%|          | 44/100000 [00:05<4:05:17,  6.79trial/s, best loss: inf]\n",
      "  0%|          | 45/100000 [00:05<4:01:02,  6.91trial/s, best loss: inf]\n",
      "  0%|          | 46/100000 [00:05<3:58:28,  6.99trial/s, best loss: inf]\n",
      "  0%|          | 47/100000 [00:05<3:56:46,  7.04trial/s, best loss: inf]\n",
      "  0%|          | 48/100000 [00:06<3:57:59,  7.00trial/s, best loss: inf]\n",
      "  0%|          | 49/100000 [00:06<4:02:40,  6.86trial/s, best loss: inf]\n",
      "  0%|          | 50/100000 [00:06<4:02:24,  6.87trial/s, best loss: inf]\n",
      "  0%|          | 51/100000 [00:06<4:02:38,  6.87trial/s, best loss: inf]\n",
      "  0%|          | 52/100000 [00:06<4:03:34,  6.84trial/s, best loss: inf]\n",
      "  0%|          | 53/100000 [00:06<4:04:56,  6.80trial/s, best loss: inf]\n",
      "  0%|          | 54/100000 [00:07<4:24:45,  6.29trial/s, best loss: inf]\n",
      "  0%|          | 55/100000 [00:07<4:21:43,  6.36trial/s, best loss: inf]\n",
      "  0%|          | 56/100000 [00:07<4:20:02,  6.41trial/s, best loss: inf]\n",
      "  0%|          | 57/100000 [00:07<4:16:17,  6.50trial/s, best loss: inf]\n",
      "  0%|          | 58/100000 [00:07<4:13:35,  6.57trial/s, best loss: inf]\n",
      "  0%|          | 59/100000 [00:07<4:12:00,  6.61trial/s, best loss: inf]\n",
      "  0%|          | 60/100000 [00:07<4:11:47,  6.62trial/s, best loss: inf]\n",
      "  0%|          | 61/100000 [00:08<4:42:50,  5.89trial/s, best loss: inf]\n",
      "  0%|          | 62/100000 [00:08<4:37:27,  6.00trial/s, best loss: inf]\n",
      "  0%|          | 63/100000 [00:08<4:32:21,  6.12trial/s, best loss: inf]\n",
      "  0%|          | 64/100000 [00:08<4:29:28,  6.18trial/s, best loss: inf]\n",
      "  0%|          | 65/100000 [00:08<4:26:21,  6.25trial/s, best loss: inf]\n",
      "  0%|          | 66/100000 [00:08<4:47:13,  5.80trial/s, best loss: inf]\n",
      "  0%|          | 67/100000 [00:09<4:39:42,  5.95trial/s, best loss: inf]\n",
      "  0%|          | 68/100000 [00:09<4:37:11,  6.01trial/s, best loss: inf]\n",
      "  0%|          | 69/100000 [00:09<4:31:14,  6.14trial/s, best loss: inf]\n",
      "  0%|          | 70/100000 [00:09<4:28:33,  6.20trial/s, best loss: inf]\n",
      "  0%|          | 71/100000 [00:09<4:29:47,  6.17trial/s, best loss: inf]\n",
      "  0%|          | 72/100000 [00:09<4:30:11,  6.16trial/s, best loss: inf]\n",
      "  0%|          | 73/100000 [00:10<4:49:42,  5.75trial/s, best loss: inf]\n",
      "  0%|          | 74/100000 [00:10<4:42:50,  5.89trial/s, best loss: inf]\n",
      "  0%|          | 75/100000 [00:10<4:37:11,  6.01trial/s, best loss: inf]\n",
      "  0%|          | 76/100000 [00:10<4:35:29,  6.05trial/s, best loss: inf]\n",
      "  0%|          | 77/100000 [00:10<4:54:07,  5.66trial/s, best loss: inf]\n",
      "  0%|          | 78/100000 [00:10<4:47:40,  5.79trial/s, best loss: inf]\n",
      "  0%|          | 79/100000 [00:11<5:09:10,  5.39trial/s, best loss: inf]\n",
      "  0%|          | 80/100000 [00:11<4:59:58,  5.55trial/s, best loss: inf]\n",
      "  0%|          | 81/100000 [00:11<4:52:17,  5.70trial/s, best loss: inf]\n",
      "  0%|          | 82/100000 [00:11<4:46:48,  5.81trial/s, best loss: inf]\n",
      "  0%|          | 83/100000 [00:11<4:44:03,  5.86trial/s, best loss: inf]\n",
      "  0%|          | 84/100000 [00:12<4:38:46,  5.97trial/s, best loss: inf]\n",
      "  0%|          | 85/100000 [00:12<4:38:15,  5.98trial/s, best loss: inf]\n",
      "  0%|          | 86/100000 [00:12<4:40:17,  5.94trial/s, best loss: inf]\n",
      "  0%|          | 87/100000 [00:12<4:42:01,  5.90trial/s, best loss: inf]\n",
      "  0%|          | 88/100000 [00:12<4:42:34,  5.89trial/s, best loss: inf]\n",
      "  0%|          | 89/100000 [00:12<5:04:49,  5.46trial/s, best loss: inf]\n",
      "  0%|          | 90/100000 [00:13<4:59:08,  5.57trial/s, best loss: inf]\n",
      "  0%|          | 91/100000 [00:13<4:56:59,  5.61trial/s, best loss: inf]\n",
      "  0%|          | 92/100000 [00:13<4:52:39,  5.69trial/s, best loss: inf]\n",
      "  0%|          | 93/100000 [00:13<4:48:49,  5.77trial/s, best loss: inf]\n",
      "  0%|          | 94/100000 [00:13<4:44:41,  5.85trial/s, best loss: inf]\n",
      "  0%|          | 95/100000 [00:13<4:42:21,  5.90trial/s, best loss: inf]\n",
      "  0%|          | 96/100000 [00:14<4:41:24,  5.92trial/s, best loss: inf]\n",
      "  0%|          | 97/100000 [00:14<4:43:01,  5.88trial/s, best loss: inf]\n",
      "  0%|          | 98/100000 [00:14<4:43:10,  5.88trial/s, best loss: inf]\n",
      "  0%|          | 99/100000 [00:14<5:14:18,  5.30trial/s, best loss: inf]\n",
      "  0%|          | 100/100000 [00:14<5:08:30,  5.40trial/s, best loss: inf]\n",
      "  0%|          | 101/100000 [00:15<5:05:53,  5.44trial/s, best loss: inf]\n",
      "  0%|          | 102/100000 [00:15<5:02:28,  5.50trial/s, best loss: inf]\n",
      "  0%|          | 103/100000 [00:15<4:58:22,  5.58trial/s, best loss: inf]\n",
      "  0%|          | 104/100000 [00:15<4:55:51,  5.63trial/s, best loss: inf]\n",
      "  0%|          | 105/100000 [00:15<4:55:26,  5.64trial/s, best loss: inf]\n",
      "  0%|          | 106/100000 [00:15<4:56:33,  5.61trial/s, best loss: inf]\n",
      "  0%|          | 107/100000 [00:16<4:53:34,  5.67trial/s, best loss: inf]\n",
      "  0%|          | 108/100000 [00:16<4:51:32,  5.71trial/s, best loss: inf]\n",
      "  0%|          | 109/100000 [00:16<4:50:49,  5.72trial/s, best loss: inf]\n",
      "  0%|          | 110/100000 [00:16<4:52:03,  5.70trial/s, best loss: inf]\n",
      "  0%|          | 111/100000 [00:16<4:55:19,  5.64trial/s, best loss: inf]\n",
      "  0%|          | 112/100000 [00:16<5:19:20,  5.21trial/s, best loss: inf]\n",
      "  0%|          | 113/100000 [00:17<5:35:59,  4.95trial/s, best loss: inf]\n",
      "  0%|          | 114/100000 [00:17<5:26:39,  5.10trial/s, best loss: inf]\n",
      "  0%|          | 115/100000 [00:17<5:20:06,  5.20trial/s, best loss: inf]\n",
      "  0%|          | 116/100000 [00:17<5:14:48,  5.29trial/s, best loss: inf]\n",
      "  0%|          | 117/100000 [00:17<5:11:48,  5.34trial/s, best loss: inf]\n",
      "  0%|          | 118/100000 [00:18<5:09:16,  5.38trial/s, best loss: inf]\n",
      "  0%|          | 119/100000 [00:18<5:08:26,  5.40trial/s, best loss: inf]\n",
      "  0%|          | 120/100000 [00:18<5:45:06,  4.82trial/s, best loss: inf]\n",
      "  0%|          | 121/100000 [00:18<5:36:22,  4.95trial/s, best loss: inf]\n",
      "  0%|          | 122/100000 [00:18<5:28:32,  5.07trial/s, best loss: inf]\n",
      "  0%|          | 123/100000 [00:19<5:24:56,  5.12trial/s, best loss: inf]\n",
      "  0%|          | 124/100000 [00:19<5:21:10,  5.18trial/s, best loss: inf]\n",
      "  0%|          | 125/100000 [00:19<5:18:42,  5.22trial/s, best loss: inf]\n",
      "  0%|          | 126/100000 [00:19<5:18:15,  5.23trial/s, best loss: inf]\n",
      "  0%|          | 127/100000 [00:19<5:14:04,  5.30trial/s, best loss: inf]\n",
      "  0%|          | 128/100000 [00:20<5:31:54,  5.02trial/s, best loss: inf]\n",
      "  0%|          | 129/100000 [00:20<5:28:19,  5.07trial/s, best loss: inf]\n",
      "  0%|          | 130/100000 [00:20<5:28:16,  5.07trial/s, best loss: inf]\n",
      "  0%|          | 131/100000 [00:20<5:25:24,  5.11trial/s, best loss: inf]\n",
      "  0%|          | 132/100000 [00:20<5:21:21,  5.18trial/s, best loss: inf]\n",
      "  0%|          | 133/100000 [00:21<5:20:39,  5.19trial/s, best loss: inf]\n",
      "  0%|          | 134/100000 [00:21<5:19:29,  5.21trial/s, best loss: inf]\n",
      "  0%|          | 135/100000 [00:21<5:21:07,  5.18trial/s, best loss: inf]\n",
      "  0%|          | 136/100000 [00:21<5:20:40,  5.19trial/s, best loss: inf]\n",
      "  0%|          | 137/100000 [00:21<5:20:31,  5.19trial/s, best loss: inf]\n",
      "  0%|          | 138/100000 [00:22<5:20:05,  5.20trial/s, best loss: inf]\n",
      "  0%|          | 139/100000 [00:22<6:04:03,  4.57trial/s, best loss: inf]\n",
      "  0%|          | 140/100000 [00:22<6:16:12,  4.42trial/s, best loss: inf]\n",
      "  0%|          | 141/100000 [00:22<6:26:09,  4.31trial/s, best loss: inf]\n",
      "  0%|          | 142/100000 [00:23<6:11:42,  4.48trial/s, best loss: inf]\n",
      "  0%|          | 143/100000 [00:23<6:01:05,  4.61trial/s, best loss: inf]\n",
      "  0%|          | 144/100000 [00:23<5:53:55,  4.70trial/s, best loss: inf]\n",
      "  0%|          | 145/100000 [00:23<5:49:23,  4.76trial/s, best loss: inf]\n",
      "  0%|          | 146/100000 [00:23<5:44:52,  4.83trial/s, best loss: inf]\n",
      "  0%|          | 147/100000 [00:24<5:41:09,  4.88trial/s, best loss: inf]\n",
      "  0%|          | 148/100000 [00:24<5:44:20,  4.83trial/s, best loss: inf]\n",
      "  0%|          | 149/100000 [00:24<5:41:13,  4.88trial/s, best loss: inf]\n",
      "  0%|          | 150/100000 [00:24<5:40:27,  4.89trial/s, best loss: inf]\n",
      "  0%|          | 151/100000 [00:24<5:40:49,  4.88trial/s, best loss: inf]\n",
      "  0%|          | 152/100000 [00:25<5:40:55,  4.88trial/s, best loss: inf]\n",
      "  0%|          | 153/100000 [00:25<6:51:59,  4.04trial/s, best loss: inf]\n",
      "  0%|          | 154/100000 [00:25<6:37:28,  4.19trial/s, best loss: inf]\n",
      "  0%|          | 155/100000 [00:25<6:19:23,  4.39trial/s, best loss: inf]\n",
      "  0%|          | 156/100000 [00:26<6:04:49,  4.56trial/s, best loss: inf]\n",
      "  0%|          | 157/100000 [00:26<6:19:39,  4.38trial/s, best loss: inf]\n",
      "  0%|          | 158/100000 [00:26<6:09:37,  4.50trial/s, best loss: inf]\n",
      "  0%|          | 159/100000 [00:26<6:03:36,  4.58trial/s, best loss: inf]\n",
      "  0%|          | 160/100000 [00:26<5:55:38,  4.68trial/s, best loss: inf]\n",
      "  0%|          | 161/100000 [00:27<5:53:26,  4.71trial/s, best loss: inf]\n",
      "  0%|          | 162/100000 [00:27<5:50:42,  4.74trial/s, best loss: inf]\n",
      "  0%|          | 163/100000 [00:27<5:49:41,  4.76trial/s, best loss: inf]\n",
      "  0%|          | 164/100000 [00:27<5:47:08,  4.79trial/s, best loss: inf]\n",
      "  0%|          | 165/100000 [00:27<6:08:03,  4.52trial/s, best loss: inf]\n",
      "  0%|          | 166/100000 [00:28<5:59:49,  4.62trial/s, best loss: inf]\n",
      "  0%|          | 167/100000 [00:28<6:10:30,  4.49trial/s, best loss: inf]\n",
      "  0%|          | 168/100000 [00:28<6:03:35,  4.58trial/s, best loss: inf]\n",
      "  0%|          | 169/100000 [00:28<5:56:22,  4.67trial/s, best loss: inf]\n",
      "  0%|          | 170/100000 [00:29<6:14:18,  4.45trial/s, best loss: inf]\n",
      "  0%|          | 171/100000 [00:29<6:03:58,  4.57trial/s, best loss: inf]\n",
      "  0%|          | 172/100000 [00:29<6:02:01,  4.60trial/s, best loss: inf]\n",
      "  0%|          | 173/100000 [00:29<5:57:41,  4.65trial/s, best loss: inf]\n",
      "  0%|          | 174/100000 [00:29<5:54:49,  4.69trial/s, best loss: inf]\n",
      "  0%|          | 175/100000 [00:30<5:55:39,  4.68trial/s, best loss: inf]\n",
      "  0%|          | 176/100000 [00:30<6:15:15,  4.43trial/s, best loss: inf]\n",
      "  0%|          | 177/100000 [00:30<6:07:27,  4.53trial/s, best loss: inf]\n",
      "  0%|          | 178/100000 [00:30<6:01:00,  4.61trial/s, best loss: inf]\n",
      "  0%|          | 179/100000 [00:31<5:56:10,  4.67trial/s, best loss: inf]\n",
      "  0%|          | 180/100000 [00:31<5:57:17,  4.66trial/s, best loss: inf]\n",
      "  0%|          | 181/100000 [00:31<5:57:48,  4.65trial/s, best loss: inf]\n",
      "  0%|          | 182/100000 [00:31<5:57:15,  4.66trial/s, best loss: inf]\n",
      "  0%|          | 183/100000 [00:31<5:55:50,  4.68trial/s, best loss: inf]\n",
      "  0%|          | 184/100000 [00:32<6:26:30,  4.30trial/s, best loss: inf]\n",
      "  0%|          | 185/100000 [00:32<6:18:53,  4.39trial/s, best loss: inf]\n",
      "  0%|          | 186/100000 [00:32<6:12:54,  4.46trial/s, best loss: inf]\n",
      "  0%|          | 187/100000 [00:32<6:08:44,  4.51trial/s, best loss: inf]\n",
      "  0%|          | 188/100000 [00:33<6:11:02,  4.48trial/s, best loss: inf]\n",
      "  0%|          | 189/100000 [00:33<6:14:09,  4.45trial/s, best loss: inf]\n",
      "  0%|          | 190/100000 [00:33<6:11:35,  4.48trial/s, best loss: inf]\n",
      "  0%|          | 191/100000 [00:33<6:31:58,  4.24trial/s, best loss: inf]\n",
      "  0%|          | 192/100000 [00:33<6:24:50,  4.32trial/s, best loss: inf]\n",
      "  0%|          | 193/100000 [00:34<6:23:13,  4.34trial/s, best loss: inf]\n",
      "  0%|          | 194/100000 [00:34<6:20:18,  4.37trial/s, best loss: inf]\n",
      "  0%|          | 195/100000 [00:34<6:15:02,  4.44trial/s, best loss: inf]\n",
      "  0%|          | 196/100000 [00:34<6:15:41,  4.43trial/s, best loss: inf]\n",
      "  0%|          | 197/100000 [00:35<6:14:40,  4.44trial/s, best loss: inf]\n",
      "  0%|          | 198/100000 [00:35<6:37:38,  4.18trial/s, best loss: inf]\n",
      "  0%|          | 199/100000 [00:35<6:36:12,  4.20trial/s, best loss: inf]\n",
      "  0%|          | 200/100000 [00:35<6:28:40,  4.28trial/s, best loss: inf]\n",
      "  0%|          | 201/100000 [00:36<6:30:21,  4.26trial/s, best loss: inf]\n",
      "  0%|          | 202/100000 [00:36<6:28:59,  4.28trial/s, best loss: inf]\n",
      "  0%|          | 203/100000 [00:36<6:27:56,  4.29trial/s, best loss: inf]\n",
      "  0%|          | 204/100000 [00:36<6:26:28,  4.30trial/s, best loss: inf]\n",
      "  0%|          | 205/100000 [00:36<6:29:52,  4.27trial/s, best loss: inf]\n",
      "  0%|          | 206/100000 [00:37<7:01:09,  3.95trial/s, best loss: inf]\n",
      "  0%|          | 207/100000 [00:37<6:54:58,  4.01trial/s, best loss: inf]\n",
      "  0%|          | 208/100000 [00:37<6:47:38,  4.08trial/s, best loss: inf]\n",
      "  0%|          | 209/100000 [00:37<6:41:24,  4.14trial/s, best loss: inf]\n",
      "  0%|          | 210/100000 [00:38<6:36:07,  4.20trial/s, best loss: inf]\n",
      "  0%|          | 211/100000 [00:38<6:37:51,  4.18trial/s, best loss: inf]\n",
      "  0%|          | 212/100000 [00:38<7:22:44,  3.76trial/s, best loss: inf]\n",
      "  0%|          | 213/100000 [00:39<7:08:11,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 214/100000 [00:39<6:55:00,  4.01trial/s, best loss: inf]\n",
      "  0%|          | 215/100000 [00:39<6:47:38,  4.08trial/s, best loss: inf]\n",
      "  0%|          | 216/100000 [00:39<6:40:58,  4.15trial/s, best loss: inf]\n",
      "  0%|          | 217/100000 [00:39<6:35:01,  4.21trial/s, best loss: inf]\n",
      "  0%|          | 218/100000 [00:40<6:32:51,  4.23trial/s, best loss: inf]\n",
      "  0%|          | 219/100000 [00:40<6:33:05,  4.23trial/s, best loss: inf]\n",
      "  0%|          | 220/100000 [00:40<6:30:59,  4.25trial/s, best loss: inf]\n",
      "  0%|          | 221/100000 [00:40<6:28:39,  4.28trial/s, best loss: inf]\n",
      "  0%|          | 222/100000 [00:41<6:27:26,  4.29trial/s, best loss: inf]\n",
      "  0%|          | 223/100000 [00:41<6:30:06,  4.26trial/s, best loss: inf]\n",
      "  0%|          | 224/100000 [00:41<6:36:27,  4.19trial/s, best loss: inf]\n",
      "  0%|          | 225/100000 [00:41<7:01:19,  3.95trial/s, best loss: inf]\n",
      "  0%|          | 226/100000 [00:42<7:17:34,  3.80trial/s, best loss: inf]\n",
      "  0%|          | 227/100000 [00:42<7:08:14,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 228/100000 [00:42<7:02:11,  3.94trial/s, best loss: inf]\n",
      "  0%|          | 229/100000 [00:42<6:57:34,  3.98trial/s, best loss: inf]\n",
      "  0%|          | 230/100000 [00:43<6:54:13,  4.01trial/s, best loss: inf]\n",
      "  0%|          | 231/100000 [00:43<7:00:10,  3.96trial/s, best loss: inf]\n",
      "  0%|          | 232/100000 [00:43<7:08:17,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 233/100000 [00:43<7:19:40,  3.78trial/s, best loss: inf]\n",
      "  0%|          | 234/100000 [00:44<7:12:33,  3.84trial/s, best loss: inf]\n",
      "  0%|          | 235/100000 [00:44<7:04:43,  3.91trial/s, best loss: inf]\n",
      "  0%|          | 236/100000 [00:44<7:04:08,  3.92trial/s, best loss: inf]\n",
      "  0%|          | 237/100000 [00:44<7:00:40,  3.95trial/s, best loss: inf]\n",
      "  0%|          | 238/100000 [00:45<7:17:36,  3.80trial/s, best loss: inf]\n",
      "  0%|          | 239/100000 [00:45<7:39:34,  3.62trial/s, best loss: inf]\n",
      "  0%|          | 240/100000 [00:45<7:31:31,  3.68trial/s, best loss: inf]\n",
      "  0%|          | 241/100000 [00:46<7:18:42,  3.79trial/s, best loss: inf]\n",
      "  0%|          | 242/100000 [00:46<7:09:57,  3.87trial/s, best loss: inf]\n",
      "  0%|          | 243/100000 [00:46<7:05:20,  3.91trial/s, best loss: inf]\n",
      "  0%|          | 244/100000 [00:46<7:10:34,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 245/100000 [00:47<7:11:13,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 246/100000 [00:47<7:37:51,  3.63trial/s, best loss: inf]\n",
      "  0%|          | 247/100000 [00:47<7:28:32,  3.71trial/s, best loss: inf]\n",
      "  0%|          | 248/100000 [00:47<7:23:47,  3.75trial/s, best loss: inf]\n",
      "  0%|          | 249/100000 [00:48<7:21:56,  3.76trial/s, best loss: inf]\n",
      "  0%|          | 250/100000 [00:48<8:14:04,  3.36trial/s, best loss: inf]\n",
      "  0%|          | 251/100000 [00:48<7:56:04,  3.49trial/s, best loss: inf]\n",
      "  0%|          | 252/100000 [00:49<7:45:10,  3.57trial/s, best loss: inf]\n",
      "  0%|          | 253/100000 [00:49<9:10:15,  3.02trial/s, best loss: inf]\n",
      "  0%|          | 254/100000 [00:49<8:57:51,  3.09trial/s, best loss: inf]\n",
      "  0%|          | 255/100000 [00:50<8:29:48,  3.26trial/s, best loss: inf]\n",
      "  0%|          | 256/100000 [00:50<8:12:32,  3.38trial/s, best loss: inf]\n",
      "  0%|          | 257/100000 [00:50<8:00:30,  3.46trial/s, best loss: inf]\n",
      "  0%|          | 258/100000 [00:50<7:48:12,  3.55trial/s, best loss: inf]\n",
      "  0%|          | 259/100000 [00:51<7:36:48,  3.64trial/s, best loss: inf]\n",
      "  0%|          | 260/100000 [00:51<7:49:33,  3.54trial/s, best loss: inf]\n",
      "  0%|          | 261/100000 [00:51<8:04:03,  3.43trial/s, best loss: inf]\n",
      "  0%|          | 262/100000 [00:52<7:50:36,  3.53trial/s, best loss: inf]\n",
      "  0%|          | 263/100000 [00:52<7:37:55,  3.63trial/s, best loss: inf]\n",
      "  0%|          | 264/100000 [00:52<7:25:21,  3.73trial/s, best loss: inf]\n",
      "  0%|          | 265/100000 [00:52<7:19:45,  3.78trial/s, best loss: inf]\n",
      "  0%|          | 266/100000 [00:53<7:15:29,  3.82trial/s, best loss: inf]\n",
      "  0%|          | 267/100000 [00:53<7:14:11,  3.83trial/s, best loss: inf]\n",
      "  0%|          | 268/100000 [00:53<7:13:57,  3.83trial/s, best loss: inf]\n",
      "  0%|          | 269/100000 [00:53<7:11:27,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 270/100000 [00:54<7:11:20,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 271/100000 [00:54<7:12:20,  3.84trial/s, best loss: inf]\n",
      "  0%|          | 272/100000 [00:54<7:17:07,  3.80trial/s, best loss: inf]\n",
      "  0%|          | 273/100000 [00:54<7:21:20,  3.77trial/s, best loss: inf]\n",
      "  0%|          | 274/100000 [00:55<7:20:37,  3.77trial/s, best loss: inf]\n",
      "  0%|          | 275/100000 [00:55<8:32:43,  3.24trial/s, best loss: inf]\n",
      "  0%|          | 276/100000 [00:55<8:34:01,  3.23trial/s, best loss: inf]\n",
      "  0%|          | 277/100000 [00:56<8:14:01,  3.36trial/s, best loss: inf]\n",
      "  0%|          | 278/100000 [00:56<8:48:24,  3.15trial/s, best loss: inf]\n",
      "  0%|          | 279/100000 [00:56<8:31:30,  3.25trial/s, best loss: inf]\n",
      "  0%|          | 279/100000 [00:56<5:38:18,  4.91trial/s, best loss: inf]\n",
      "2025-06-11 16:36:53,528 - SimultaneousTuner - Final graph: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "2025-06-11 16:36:53,530 - SimultaneousTuner - Final metric: 0.813\n",
      "2025-06-11 16:36:53,538 - ApiComposer - Hyperparameters tuning finished\n",
      "2025-06-11 16:36:53,675 - ApiComposer - Model generation finished\n",
      "2025-06-11 16:36:54,176 - FEDOT logger - Final pipeline was fitted\n",
      "2025-06-11 16:36:54,176 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "Model metrics:  {'accuracy': 0.788}\n",
      "Predictions shape: (418, 1)\n",
      "Model Performance on Test Set: {'accuracy': 0.788}\n",
      "\n",
      "Sample submission file path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/sample_submission.csv\n",
      "2025-06-11 16:36:55,125 - Test failed: Error validating submission format: [Errno 2] No such file or directory: '/Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/sample_submission.csv'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:36:55,128 - FEDOTLLM - INFO - Running fix solution\n",
      "2025-06-11 16:37:10,925 - FEDOTLLM - INFO - Running insert templates\n",
      "2025-06-11 16:37:10,953 - FEDOTLLM - INFO - Running evaluate\n",
      "2025-06-11 16:38:14,539 - FEDOTLLM - ERROR - Too many fix tries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:38:14,539 - Too many fix tries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:38:14,542 - FEDOTLLM - INFO - Running tests\n",
      "2025-06-11 16:38:14,543 - FEDOTLLM - INFO - Test passed: {'accuracy': 0.788}\n",
      "2025-06-11 16:38:14,556 - FEDOTLLM - INFO - Test passed: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "2025-06-11 16:38:14,556 - FEDOTLLM - INFO - Test passed: Submission file exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DEBUG: RAW OUTPUT\n",
      " Files and directories:\n",
      "Dataset Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition\n",
      "Workspace Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output\n",
      "Pipeline Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/pipeline\n",
      "Submission Path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/output/submission.csv\n",
      "Train File: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/train.csv\n",
      "Test File: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/test.csv\n",
      "Sample Submission File: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/gender_submission.csv\n",
      "2025-06-11 16:37:14,368 - ApiComposer - Initial pipeline was fitted in 0.6 sec.\n",
      "2025-06-11 16:37:14,368 - ApiComposer - Taking into account n_folds=5, estimated fit time for initial assumption is 3.0 sec.\n",
      "2025-06-11 16:37:14,371 - ApiComposer - AutoML configured. Parameters tuning: True. Time limit: 1.0 min. Set of candidate models: ['bernb', 'catboost', 'dt', 'fast_ica', 'isolation_forest_class', 'knn', 'lgbm', 'logit', 'mlp', 'normalization', 'pca', 'poly_features', 'qda', 'resample', 'rf', 'scaling', 'xgboost'].\n",
      "2025-06-11 16:37:14,428 - ApiComposer - Timeout is too small for composing and is skipped because fit_time is 2.99615 sec.\n",
      "2025-06-11 16:37:14,433 - ApiComposer - Hyperparameters tuning started with 1 min. timeout\n",
      "2025-06-11 16:37:16,848 - SimultaneousTuner - Initial graph: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1} \n",
      "Initial metric: [0.813]\n",
      "\n",
      "  0%|          | 0/100000 [00:00<?, ?trial/s, best loss=?]\n",
      "  0%|          | 2/100000 [00:00<3:21:13,  8.28trial/s, best loss: inf]\n",
      "  0%|          | 4/100000 [00:00<2:50:18,  9.79trial/s, best loss: inf]\n",
      "  0%|          | 6/100000 [00:00<2:37:53, 10.55trial/s, best loss: inf]\n",
      "  0%|          | 8/100000 [00:00<2:30:57, 11.04trial/s, best loss: inf]\n",
      "  0%|          | 10/100000 [00:00<2:26:53, 11.35trial/s, best loss: inf]\n",
      "  0%|          | 12/100000 [00:01<2:24:50, 11.51trial/s, best loss: inf]\n",
      "  0%|          | 14/100000 [00:01<2:24:52, 11.50trial/s, best loss: inf]\n",
      "  0%|          | 16/100000 [00:01<2:23:54, 11.58trial/s, best loss: inf]\n",
      "  0%|          | 18/100000 [00:01<2:23:02, 11.65trial/s, best loss: inf]\n",
      "  0%|          | 20/100000 [00:01<2:23:50, 11.58trial/s, best loss: inf]\n",
      "  0%|          | 22/100000 [00:02<2:45:24, 10.07trial/s, best loss: inf]\n",
      "  0%|          | 24/100000 [00:02<3:01:38,  9.17trial/s, best loss: inf]\n",
      "  0%|          | 25/100000 [00:02<3:08:08,  8.86trial/s, best loss: inf]\n",
      "  0%|          | 26/100000 [00:02<3:14:07,  8.58trial/s, best loss: inf]\n",
      "  0%|          | 27/100000 [00:02<3:19:48,  8.34trial/s, best loss: inf]\n",
      "  0%|          | 28/100000 [00:02<3:23:53,  8.17trial/s, best loss: inf]\n",
      "  0%|          | 29/100000 [00:02<3:43:49,  7.44trial/s, best loss: inf]\n",
      "  0%|          | 30/100000 [00:03<3:41:53,  7.51trial/s, best loss: inf]\n",
      "  0%|          | 31/100000 [00:03<3:40:30,  7.56trial/s, best loss: inf]\n",
      "  0%|          | 32/100000 [00:03<3:40:04,  7.57trial/s, best loss: inf]\n",
      "  0%|          | 33/100000 [00:03<3:42:03,  7.50trial/s, best loss: inf]\n",
      "  0%|          | 34/100000 [00:03<3:41:52,  7.51trial/s, best loss: inf]\n",
      "  0%|          | 35/100000 [00:03<3:41:06,  7.53trial/s, best loss: inf]\n",
      "  0%|          | 36/100000 [00:03<3:40:34,  7.55trial/s, best loss: inf]\n",
      "  0%|          | 37/100000 [00:04<3:43:24,  7.46trial/s, best loss: inf]\n",
      "  0%|          | 38/100000 [00:04<3:44:08,  7.43trial/s, best loss: inf]\n",
      "  0%|          | 39/100000 [00:04<3:45:08,  7.40trial/s, best loss: inf]\n",
      "  0%|          | 40/100000 [00:04<3:45:01,  7.40trial/s, best loss: inf]\n",
      "  0%|          | 41/100000 [00:04<3:45:49,  7.38trial/s, best loss: inf]\n",
      "  0%|          | 42/100000 [00:04<4:07:39,  6.73trial/s, best loss: inf]\n",
      "  0%|          | 43/100000 [00:04<4:05:06,  6.80trial/s, best loss: inf]\n",
      "  0%|          | 44/100000 [00:05<4:03:42,  6.84trial/s, best loss: inf]\n",
      "  0%|          | 45/100000 [00:05<4:00:39,  6.92trial/s, best loss: inf]\n",
      "  0%|          | 46/100000 [00:05<4:07:39,  6.73trial/s, best loss: inf]\n",
      "  0%|          | 47/100000 [00:05<4:07:47,  6.72trial/s, best loss: inf]\n",
      "  0%|          | 48/100000 [00:05<4:05:03,  6.80trial/s, best loss: inf]\n",
      "  0%|          | 49/100000 [00:05<4:04:45,  6.81trial/s, best loss: inf]\n",
      "  0%|          | 50/100000 [00:05<4:02:57,  6.86trial/s, best loss: inf]\n",
      "  0%|          | 51/100000 [00:06<4:01:58,  6.88trial/s, best loss: inf]\n",
      "  0%|          | 52/100000 [00:06<4:01:34,  6.90trial/s, best loss: inf]\n",
      "  0%|          | 53/100000 [00:06<4:17:14,  6.48trial/s, best loss: inf]\n",
      "  0%|          | 54/100000 [00:06<4:37:45,  6.00trial/s, best loss: inf]\n",
      "  0%|          | 55/100000 [00:06<4:27:57,  6.22trial/s, best loss: inf]\n",
      "  0%|          | 56/100000 [00:06<4:25:52,  6.27trial/s, best loss: inf]\n",
      "  0%|          | 57/100000 [00:07<4:20:23,  6.40trial/s, best loss: inf]\n",
      "  0%|          | 58/100000 [00:07<4:15:20,  6.52trial/s, best loss: inf]\n",
      "  0%|          | 59/100000 [00:07<4:16:12,  6.50trial/s, best loss: inf]\n",
      "  0%|          | 60/100000 [00:07<4:17:13,  6.48trial/s, best loss: inf]\n",
      "  0%|          | 61/100000 [00:07<4:17:00,  6.48trial/s, best loss: inf]\n",
      "  0%|          | 62/100000 [00:07<4:19:25,  6.42trial/s, best loss: inf]\n",
      "  0%|          | 63/100000 [00:07<4:20:07,  6.40trial/s, best loss: inf]\n",
      "  0%|          | 64/100000 [00:08<4:19:02,  6.43trial/s, best loss: inf]\n",
      "  0%|          | 65/100000 [00:08<4:15:20,  6.52trial/s, best loss: inf]\n",
      "  0%|          | 66/100000 [00:08<4:34:38,  6.06trial/s, best loss: inf]\n",
      "  0%|          | 67/100000 [00:08<4:46:43,  5.81trial/s, best loss: inf]\n",
      "  0%|          | 68/100000 [00:08<4:38:49,  5.97trial/s, best loss: inf]\n",
      "  0%|          | 69/100000 [00:08<4:35:20,  6.05trial/s, best loss: inf]\n",
      "  0%|          | 70/100000 [00:09<4:30:37,  6.15trial/s, best loss: inf]\n",
      "  0%|          | 71/100000 [00:09<4:29:54,  6.17trial/s, best loss: inf]\n",
      "  0%|          | 72/100000 [00:09<4:28:45,  6.20trial/s, best loss: inf]\n",
      "  0%|          | 73/100000 [00:09<4:24:59,  6.28trial/s, best loss: inf]\n",
      "  0%|          | 74/100000 [00:09<4:25:31,  6.27trial/s, best loss: inf]\n",
      "  0%|          | 75/100000 [00:09<4:28:32,  6.20trial/s, best loss: inf]\n",
      "  0%|          | 76/100000 [00:10<4:28:52,  6.19trial/s, best loss: inf]\n",
      "  0%|          | 77/100000 [00:10<4:50:51,  5.73trial/s, best loss: inf]\n",
      "  0%|          | 78/100000 [00:10<4:45:59,  5.82trial/s, best loss: inf]\n",
      "  0%|          | 79/100000 [00:10<4:41:58,  5.91trial/s, best loss: inf]\n",
      "  0%|          | 80/100000 [00:10<5:00:54,  5.53trial/s, best loss: inf]\n",
      "  0%|          | 81/100000 [00:11<4:54:17,  5.66trial/s, best loss: inf]\n",
      "  0%|          | 82/100000 [00:11<4:50:41,  5.73trial/s, best loss: inf]\n",
      "  0%|          | 83/100000 [00:11<4:47:57,  5.78trial/s, best loss: inf]\n",
      "  0%|          | 84/100000 [00:11<4:46:00,  5.82trial/s, best loss: inf]\n",
      "  0%|          | 85/100000 [00:11<4:42:47,  5.89trial/s, best loss: inf]\n",
      "  0%|          | 86/100000 [00:11<4:40:20,  5.94trial/s, best loss: inf]\n",
      "  0%|          | 87/100000 [00:12<4:42:44,  5.89trial/s, best loss: inf]\n",
      "  0%|          | 88/100000 [00:12<4:43:33,  5.87trial/s, best loss: inf]\n",
      "  0%|          | 89/100000 [00:12<4:41:20,  5.92trial/s, best loss: inf]\n",
      "  0%|          | 90/100000 [00:12<4:42:03,  5.90trial/s, best loss: inf]\n",
      "  0%|          | 91/100000 [00:12<4:43:16,  5.88trial/s, best loss: inf]\n",
      "  0%|          | 92/100000 [00:12<4:41:32,  5.91trial/s, best loss: inf]\n",
      "  0%|          | 93/100000 [00:13<4:38:42,  5.97trial/s, best loss: inf]\n",
      "  0%|          | 94/100000 [00:13<4:37:10,  6.01trial/s, best loss: inf]\n",
      "  0%|          | 95/100000 [00:13<4:38:59,  5.97trial/s, best loss: inf]\n",
      "  0%|          | 96/100000 [00:13<5:13:33,  5.31trial/s, best loss: inf]\n",
      "  0%|          | 97/100000 [00:13<5:07:41,  5.41trial/s, best loss: inf]\n",
      "  0%|          | 98/100000 [00:13<5:02:56,  5.50trial/s, best loss: inf]\n",
      "  0%|          | 99/100000 [00:14<4:59:52,  5.55trial/s, best loss: inf]\n",
      "  0%|          | 100/100000 [00:14<4:55:32,  5.63trial/s, best loss: inf]\n",
      "  0%|          | 101/100000 [00:14<4:56:32,  5.61trial/s, best loss: inf]\n",
      "  0%|          | 102/100000 [00:14<4:56:39,  5.61trial/s, best loss: inf]\n",
      "  0%|          | 103/100000 [00:14<4:57:09,  5.60trial/s, best loss: inf]\n",
      "  0%|          | 104/100000 [00:15<4:57:41,  5.59trial/s, best loss: inf]\n",
      "  0%|          | 105/100000 [00:15<4:57:48,  5.59trial/s, best loss: inf]\n",
      "  0%|          | 106/100000 [00:15<4:58:55,  5.57trial/s, best loss: inf]\n",
      "  0%|          | 107/100000 [00:15<4:55:21,  5.64trial/s, best loss: inf]\n",
      "  0%|          | 108/100000 [00:15<4:53:39,  5.67trial/s, best loss: inf]\n",
      "  0%|          | 109/100000 [00:15<4:52:31,  5.69trial/s, best loss: inf]\n",
      "  0%|          | 110/100000 [00:16<5:25:55,  5.11trial/s, best loss: inf]\n",
      "  0%|          | 111/100000 [00:16<5:20:05,  5.20trial/s, best loss: inf]\n",
      "  0%|          | 112/100000 [00:16<5:18:11,  5.23trial/s, best loss: inf]\n",
      "  0%|          | 113/100000 [00:16<5:34:49,  4.97trial/s, best loss: inf]\n",
      "  0%|          | 114/100000 [00:16<5:24:42,  5.13trial/s, best loss: inf]\n",
      "  0%|          | 115/100000 [00:17<5:24:23,  5.13trial/s, best loss: inf]\n",
      "  0%|          | 116/100000 [00:17<5:26:58,  5.09trial/s, best loss: inf]\n",
      "  0%|          | 117/100000 [00:17<5:22:47,  5.16trial/s, best loss: inf]\n",
      "  0%|          | 118/100000 [00:17<5:21:33,  5.18trial/s, best loss: inf]\n",
      "  0%|          | 119/100000 [00:17<5:39:13,  4.91trial/s, best loss: inf]\n",
      "  0%|          | 120/100000 [00:18<5:33:31,  4.99trial/s, best loss: inf]\n",
      "  0%|          | 121/100000 [00:18<5:26:52,  5.09trial/s, best loss: inf]\n",
      "  0%|          | 122/100000 [00:18<5:22:13,  5.17trial/s, best loss: inf]\n",
      "  0%|          | 123/100000 [00:18<5:19:15,  5.21trial/s, best loss: inf]\n",
      "  0%|          | 124/100000 [00:18<5:18:08,  5.23trial/s, best loss: inf]\n",
      "  0%|          | 125/100000 [00:19<5:15:40,  5.27trial/s, best loss: inf]\n",
      "  0%|          | 126/100000 [00:19<5:17:25,  5.24trial/s, best loss: inf]\n",
      "  0%|          | 127/100000 [00:19<5:29:03,  5.06trial/s, best loss: inf]\n",
      "  0%|          | 128/100000 [00:19<5:57:33,  4.66trial/s, best loss: inf]\n",
      "  0%|          | 129/100000 [00:19<5:49:04,  4.77trial/s, best loss: inf]\n",
      "  0%|          | 130/100000 [00:20<5:41:34,  4.87trial/s, best loss: inf]\n",
      "  0%|          | 131/100000 [00:20<5:31:56,  5.01trial/s, best loss: inf]\n",
      "  0%|          | 132/100000 [00:20<5:27:24,  5.08trial/s, best loss: inf]\n",
      "  0%|          | 133/100000 [00:20<5:24:43,  5.13trial/s, best loss: inf]\n",
      "  0%|          | 134/100000 [00:20<5:44:35,  4.83trial/s, best loss: inf]\n",
      "  0%|          | 135/100000 [00:21<5:38:05,  4.92trial/s, best loss: inf]\n",
      "  0%|          | 136/100000 [00:21<5:33:23,  4.99trial/s, best loss: inf]\n",
      "  0%|          | 137/100000 [00:21<5:28:31,  5.07trial/s, best loss: inf]\n",
      "  0%|          | 138/100000 [00:21<5:23:27,  5.15trial/s, best loss: inf]\n",
      "  0%|          | 139/100000 [00:21<5:21:15,  5.18trial/s, best loss: inf]\n",
      "  0%|          | 140/100000 [00:22<5:19:34,  5.21trial/s, best loss: inf]\n",
      "  0%|          | 141/100000 [00:22<5:37:53,  4.93trial/s, best loss: inf]\n",
      "  0%|          | 142/100000 [00:22<5:33:22,  4.99trial/s, best loss: inf]\n",
      "  0%|          | 143/100000 [00:22<5:33:45,  4.99trial/s, best loss: inf]\n",
      "  0%|          | 144/100000 [00:22<5:53:15,  4.71trial/s, best loss: inf]\n",
      "  0%|          | 145/100000 [00:23<5:48:20,  4.78trial/s, best loss: inf]\n",
      "  0%|          | 146/100000 [00:23<5:42:08,  4.86trial/s, best loss: inf]\n",
      "  0%|          | 147/100000 [00:23<5:41:51,  4.87trial/s, best loss: inf]\n",
      "  0%|          | 148/100000 [00:23<5:39:06,  4.91trial/s, best loss: inf]\n",
      "  0%|          | 149/100000 [00:23<5:35:50,  4.96trial/s, best loss: inf]\n",
      "  0%|          | 150/100000 [00:24<5:32:47,  5.00trial/s, best loss: inf]\n",
      "  0%|          | 151/100000 [00:24<5:29:24,  5.05trial/s, best loss: inf]\n",
      "  0%|          | 152/100000 [00:24<5:36:47,  4.94trial/s, best loss: inf]\n",
      "  0%|          | 153/100000 [00:24<6:16:17,  4.42trial/s, best loss: inf]\n",
      "  0%|          | 154/100000 [00:25<6:02:55,  4.59trial/s, best loss: inf]\n",
      "  0%|          | 155/100000 [00:25<5:54:59,  4.69trial/s, best loss: inf]\n",
      "  0%|          | 156/100000 [00:25<5:52:55,  4.72trial/s, best loss: inf]\n",
      "  0%|          | 157/100000 [00:25<5:48:07,  4.78trial/s, best loss: inf]\n",
      "  0%|          | 158/100000 [00:25<5:46:23,  4.80trial/s, best loss: inf]\n",
      "  0%|          | 159/100000 [00:26<5:45:24,  4.82trial/s, best loss: inf]\n",
      "  0%|          | 160/100000 [00:26<5:43:28,  4.84trial/s, best loss: inf]\n",
      "  0%|          | 161/100000 [00:26<5:40:12,  4.89trial/s, best loss: inf]\n",
      "  0%|          | 162/100000 [00:26<5:38:40,  4.91trial/s, best loss: inf]\n",
      "  0%|          | 163/100000 [00:26<5:37:39,  4.93trial/s, best loss: inf]\n",
      "  0%|          | 164/100000 [00:27<5:37:39,  4.93trial/s, best loss: inf]\n",
      "  0%|          | 165/100000 [00:27<6:35:26,  4.21trial/s, best loss: inf]\n",
      "  0%|          | 166/100000 [00:27<6:23:31,  4.34trial/s, best loss: inf]\n",
      "  0%|          | 167/100000 [00:27<6:17:51,  4.40trial/s, best loss: inf]\n",
      "  0%|          | 168/100000 [00:28<6:17:55,  4.40trial/s, best loss: inf]\n",
      "  0%|          | 169/100000 [00:28<6:10:15,  4.49trial/s, best loss: inf]\n",
      "  0%|          | 170/100000 [00:28<6:37:32,  4.19trial/s, best loss: inf]\n",
      "  0%|          | 171/100000 [00:28<6:31:05,  4.25trial/s, best loss: inf]\n",
      "  0%|          | 172/100000 [00:29<6:51:07,  4.05trial/s, best loss: inf]\n",
      "  0%|          | 173/100000 [00:29<6:53:38,  4.02trial/s, best loss: inf]\n",
      "  0%|          | 174/100000 [00:29<6:50:19,  4.05trial/s, best loss: inf]\n",
      "  0%|          | 175/100000 [00:29<7:06:18,  3.90trial/s, best loss: inf]\n",
      "  0%|          | 176/100000 [00:30<7:14:56,  3.83trial/s, best loss: inf]\n",
      "  0%|          | 177/100000 [00:30<6:54:58,  4.01trial/s, best loss: inf]\n",
      "  0%|          | 178/100000 [00:30<6:43:29,  4.12trial/s, best loss: inf]\n",
      "  0%|          | 179/100000 [00:30<6:32:39,  4.24trial/s, best loss: inf]\n",
      "  0%|          | 180/100000 [00:30<6:22:04,  4.35trial/s, best loss: inf]\n",
      "  0%|          | 181/100000 [00:31<6:44:45,  4.11trial/s, best loss: inf]\n",
      "  0%|          | 182/100000 [00:31<6:33:37,  4.23trial/s, best loss: inf]\n",
      "  0%|          | 183/100000 [00:31<6:26:21,  4.31trial/s, best loss: inf]\n",
      "  0%|          | 184/100000 [00:31<6:24:54,  4.32trial/s, best loss: inf]\n",
      "  0%|          | 185/100000 [00:32<6:23:38,  4.34trial/s, best loss: inf]\n",
      "  0%|          | 186/100000 [00:32<6:22:35,  4.35trial/s, best loss: inf]\n",
      "  0%|          | 187/100000 [00:32<6:22:08,  4.35trial/s, best loss: inf]\n",
      "  0%|          | 188/100000 [00:32<6:18:51,  4.39trial/s, best loss: inf]\n",
      "  0%|          | 189/100000 [00:33<6:46:00,  4.10trial/s, best loss: inf]\n",
      "  0%|          | 190/100000 [00:33<6:48:04,  4.08trial/s, best loss: inf]\n",
      "  0%|          | 191/100000 [00:33<6:44:36,  4.11trial/s, best loss: inf]\n",
      "  0%|          | 192/100000 [00:33<6:42:19,  4.13trial/s, best loss: inf]\n",
      "  0%|          | 193/100000 [00:34<7:02:30,  3.94trial/s, best loss: inf]\n",
      "  0%|          | 194/100000 [00:34<6:54:39,  4.01trial/s, best loss: inf]\n",
      "  0%|          | 195/100000 [00:34<6:47:15,  4.08trial/s, best loss: inf]\n",
      "  0%|          | 196/100000 [00:34<6:43:44,  4.12trial/s, best loss: inf]\n",
      "  0%|          | 197/100000 [00:35<6:43:40,  4.12trial/s, best loss: inf]\n",
      "  0%|          | 198/100000 [00:35<6:37:47,  4.18trial/s, best loss: inf]\n",
      "  0%|          | 199/100000 [00:35<6:38:25,  4.17trial/s, best loss: inf]\n",
      "  0%|          | 200/100000 [00:35<6:34:41,  4.21trial/s, best loss: inf]\n",
      "  0%|          | 201/100000 [00:35<6:32:03,  4.24trial/s, best loss: inf]\n",
      "  0%|          | 202/100000 [00:36<6:30:53,  4.26trial/s, best loss: inf]\n",
      "  0%|          | 203/100000 [00:36<6:28:21,  4.28trial/s, best loss: inf]\n",
      "  0%|          | 204/100000 [00:36<6:53:13,  4.03trial/s, best loss: inf]\n",
      "  0%|          | 205/100000 [00:36<6:47:49,  4.08trial/s, best loss: inf]\n",
      "  0%|          | 206/100000 [00:37<6:44:11,  4.12trial/s, best loss: inf]\n",
      "  0%|          | 207/100000 [00:37<6:43:53,  4.12trial/s, best loss: inf]\n",
      "  0%|          | 208/100000 [00:37<6:40:44,  4.15trial/s, best loss: inf]\n",
      "  0%|          | 209/100000 [00:37<6:34:16,  4.22trial/s, best loss: inf]\n",
      "  0%|          | 210/100000 [00:38<7:08:15,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 211/100000 [00:38<7:01:56,  3.94trial/s, best loss: inf]\n",
      "  0%|          | 212/100000 [00:38<7:18:40,  3.79trial/s, best loss: inf]\n",
      "  0%|          | 213/100000 [00:38<7:02:54,  3.93trial/s, best loss: inf]\n",
      "  0%|          | 214/100000 [00:39<6:50:07,  4.06trial/s, best loss: inf]\n",
      "  0%|          | 215/100000 [00:39<6:42:08,  4.14trial/s, best loss: inf]\n",
      "  0%|          | 216/100000 [00:39<7:10:11,  3.87trial/s, best loss: inf]\n",
      "  0%|          | 217/100000 [00:39<6:58:29,  3.97trial/s, best loss: inf]\n",
      "  0%|          | 218/100000 [00:40<6:46:44,  4.09trial/s, best loss: inf]\n",
      "  0%|          | 219/100000 [00:40<6:42:03,  4.14trial/s, best loss: inf]\n",
      "  0%|          | 220/100000 [00:40<6:38:38,  4.17trial/s, best loss: inf]\n",
      "  0%|          | 221/100000 [00:40<6:43:07,  4.13trial/s, best loss: inf]\n",
      "  0%|          | 222/100000 [00:41<6:47:29,  4.08trial/s, best loss: inf]\n",
      "  0%|          | 223/100000 [00:41<7:13:16,  3.84trial/s, best loss: inf]\n",
      "  0%|          | 224/100000 [00:41<7:03:51,  3.92trial/s, best loss: inf]\n",
      "  0%|          | 225/100000 [00:41<6:57:18,  3.98trial/s, best loss: inf]\n",
      "  0%|          | 226/100000 [00:42<7:09:46,  3.87trial/s, best loss: inf]\n",
      "  0%|          | 227/100000 [00:42<6:57:59,  3.98trial/s, best loss: inf]\n",
      "  0%|          | 228/100000 [00:42<6:49:42,  4.06trial/s, best loss: inf]\n",
      "  0%|          | 229/100000 [00:42<6:44:33,  4.11trial/s, best loss: inf]\n",
      "  0%|          | 230/100000 [00:43<6:52:08,  4.03trial/s, best loss: inf]\n",
      "  0%|          | 231/100000 [00:43<7:32:53,  3.67trial/s, best loss: inf]\n",
      "  0%|          | 232/100000 [00:43<7:21:36,  3.77trial/s, best loss: inf]\n",
      "  0%|          | 233/100000 [00:44<7:10:19,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 234/100000 [00:44<7:29:55,  3.70trial/s, best loss: inf]\n",
      "  0%|          | 235/100000 [00:44<7:19:07,  3.79trial/s, best loss: inf]\n",
      "  0%|          | 236/100000 [00:44<7:37:31,  3.63trial/s, best loss: inf]\n",
      "  0%|          | 237/100000 [00:45<7:28:11,  3.71trial/s, best loss: inf]\n",
      "  0%|          | 238/100000 [00:45<7:38:03,  3.63trial/s, best loss: inf]\n",
      "  0%|          | 239/100000 [00:45<7:57:42,  3.48trial/s, best loss: inf]\n",
      "  0%|          | 240/100000 [00:45<7:53:44,  3.51trial/s, best loss: inf]\n",
      "  0%|          | 241/100000 [00:46<7:38:35,  3.63trial/s, best loss: inf]\n",
      "  0%|          | 242/100000 [00:46<7:29:16,  3.70trial/s, best loss: inf]\n",
      "  0%|          | 243/100000 [00:46<7:21:25,  3.77trial/s, best loss: inf]\n",
      "  0%|          | 244/100000 [00:47<7:21:03,  3.77trial/s, best loss: inf]\n",
      "  0%|          | 245/100000 [00:47<7:18:55,  3.79trial/s, best loss: inf]\n",
      "  0%|          | 246/100000 [00:47<7:11:14,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 247/100000 [00:47<7:10:57,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 248/100000 [00:48<7:04:11,  3.92trial/s, best loss: inf]\n",
      "  0%|          | 249/100000 [00:48<7:01:03,  3.95trial/s, best loss: inf]\n",
      "  0%|          | 250/100000 [00:48<7:17:43,  3.80trial/s, best loss: inf]\n",
      "  0%|          | 251/100000 [00:48<7:10:28,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 252/100000 [00:49<7:07:20,  3.89trial/s, best loss: inf]\n",
      "  0%|          | 253/100000 [00:49<7:03:52,  3.92trial/s, best loss: inf]\n",
      "  0%|          | 254/100000 [00:49<7:19:32,  3.78trial/s, best loss: inf]\n",
      "  0%|          | 255/100000 [00:49<7:12:16,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 256/100000 [00:50<7:05:55,  3.90trial/s, best loss: inf]\n",
      "  0%|          | 257/100000 [00:50<7:06:42,  3.90trial/s, best loss: inf]\n",
      "  0%|          | 258/100000 [00:50<7:08:42,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 259/100000 [00:50<7:08:04,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 260/100000 [00:51<7:11:04,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 261/100000 [00:51<7:35:38,  3.65trial/s, best loss: inf]\n",
      "  0%|          | 262/100000 [00:51<7:26:29,  3.72trial/s, best loss: inf]\n",
      "  0%|          | 263/100000 [00:51<7:21:06,  3.77trial/s, best loss: inf]\n",
      "  0%|          | 264/100000 [00:52<7:17:02,  3.80trial/s, best loss: inf]\n",
      "  0%|          | 265/100000 [00:52<7:12:31,  3.84trial/s, best loss: inf]\n",
      "  0%|          | 266/100000 [00:52<7:09:59,  3.87trial/s, best loss: inf]\n",
      "  0%|          | 267/100000 [00:52<7:08:09,  3.88trial/s, best loss: inf]\n",
      "  0%|          | 268/100000 [00:53<7:10:22,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 269/100000 [00:53<7:10:10,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 270/100000 [00:53<7:10:19,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 271/100000 [00:54<7:09:59,  3.87trial/s, best loss: inf]\n",
      "  0%|          | 272/100000 [00:54<7:10:17,  3.86trial/s, best loss: inf]\n",
      "  0%|          | 273/100000 [00:54<7:11:44,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 274/100000 [00:54<7:12:16,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 275/100000 [00:55<7:11:45,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 276/100000 [00:55<7:09:53,  3.87trial/s, best loss: inf]\n",
      "  0%|          | 277/100000 [00:55<7:11:24,  3.85trial/s, best loss: inf]\n",
      "  0%|          | 278/100000 [00:55<7:13:16,  3.84trial/s, best loss: inf]\n",
      "  0%|          | 279/100000 [00:56<7:12:20,  3.84trial/s, best loss: inf]\n",
      "  0%|          | 280/100000 [00:56<7:13:28,  3.83trial/s, best loss: inf]\n",
      "  0%|          | 281/100000 [00:56<7:15:51,  3.81trial/s, best loss: inf]\n",
      "  0%|          | 281/100000 [00:56<5:35:00,  4.96trial/s, best loss: inf]\n",
      "2025-06-11 16:38:13,593 - SimultaneousTuner - Final graph: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "2025-06-11 16:38:13,595 - SimultaneousTuner - Final metric: 0.813\n",
      "2025-06-11 16:38:13,598 - ApiComposer - Hyperparameters tuning finished\n",
      "2025-06-11 16:38:13,635 - ApiComposer - Model generation finished\n",
      "2025-06-11 16:38:14,137 - FEDOT logger - Final pipeline was fitted\n",
      "2025-06-11 16:38:14,137 - FEDOT logger - Final pipeline: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "Model metrics:  {'accuracy': 0.788}\n",
      "Predictions shape: (418, 1)\n",
      "Model Performance on Test Set: {'accuracy': 0.788}\n",
      "\n",
      "Sample submission file path: /Users/aleksejlapin/Work/STABLE-FedotLLM/examples/titanic/competition/gender_submission.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:38:18,028 - FEDOTLLM - INFO - Test passed: Submission file format is correct.\n",
      "2025-06-11 16:38:18,034 - FEDOTLLM - ERROR - Too many fix tries\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:38:18,034 - Too many fix tries\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-11 16:38:18,038 - FEDOTLLM - INFO - Running extract_metrics\n",
      "2025-06-11 16:38:18,038 - FEDOTLLM - INFO - Metrics: {'accuracy': 0.788}\n",
      "2025-06-11 16:38:18,052 - FEDOTLLM - INFO - Pipeline: {'depth': 3, 'length': 5, 'nodes': [logit, catboost, scaling, xgboost, lgbm]}\n",
      "logit - {}\n",
      "catboost - {'n_jobs': 1, 'num_trees': 3000, 'learning_rate': 0.03, 'l2_leaf_reg': 0.01, 'bootstrap_type': 'Bernoulli', 'grow_policy': 'SymmetricTree', 'max_depth': 5, 'min_data_in_leaf': 1, 'one_hot_max_size': 10, 'fold_permutation_block': 1, 'boosting_type': 'Plain', 'od_type': 'Iter', 'od_wait': 100, 'max_bin': 32, 'feature_border_type': 'GreedyLogSum', 'nan_mode': 'Min', 'verbose': False, 'allow_writing_files': False, 'use_eval_set': True, 'use_best_model': True, 'enable_categorical': True}\n",
      "scaling - {}\n",
      "xgboost - {'n_jobs': 1, 'verbosity': 0, 'booster': 'gbtree', 'tree_method': 'auto', 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30}\n",
      "lgbm - {'boosting_type': 'gbdt', 'max_depth': -1, 'bagging_fraction': 0.85, 'extra_trees': False, 'enable_categorical': True, 'use_eval_set': True, 'early_stopping_rounds': 30, 'n_jobs': 1, 'verbose': -1}\n",
      "2025-06-11 16:40:26,834 - FEDOTLLM - INFO - FedotAI ainvoke: Before output translation. Source lang: ru. Content (first 100): '## **Navigating the Seas: Decoding the Titanic Survival Model**\n",
      "\n",
      "### **Overview**\n",
      "\n",
      "*   **Problem:** ...'\n",
      "2025-06-11 16:40:26,839 - FEDOTLLM - INFO - TranslatorAgent: Attempting output translation. Current source_language_for_session: ru\n",
      "2025-06-11 16:40:26,840 - FEDOTLLM - INFO - Translating output from English to ru using self.inference.query.\n",
      "2025-06-11 16:40:26,841 - FEDOTLLM - INFO - English message for output translation (first 200 chars): '## **Navigating the Seas: Decoding the Titanic Survival Model**\n",
      "\n",
      "### **Overview**\n",
      "\n",
      "*   **Problem:** We're setting sail on the Titanic's data, aiming to predict who survived the tragic sinking and who ...'\n",
      "2025-06-11 16:40:26,841 - FEDOTLLM - INFO - TranslatorAgent._translate_text: Attempting translation from 'en' to 'ru' using self.inference.query.\n",
      "2025-06-11 16:41:17,670 - FEDOTLLM - INFO - Successfully translated text from en to ru using self.inference.query.\n",
      "2025-06-11 16:41:17,688 - FEDOTLLM - INFO - FedotAI ainvoke: After output translation. Translated content (first 100): '## **Путешествие по морям: Расшифровка модели выживания на \"Титанике\"**\n",
      "\n",
      "### **Обзор**\n",
      "\n",
      "*   **Пробле...'\n",
      "2025-06-11 16:41:17,692 - FEDOTLLM - WARNING - FedotAI ainvoke: Last message is not AIMessage (type: <class 'langchain_core.messages.human.HumanMessage'>), direct content update might be insufficient or ineffective if immutable.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'messages': [HumanMessage(content=\"Build an AutoML model. The 'Titanic - Machine Learning from Disaster' challenge on Kaggle involves predicting whether a passenger survived the sinking of the Titanic using the provided datasets. The train.csv dataset includes 891 passengers with their survival status (ground truth), while test.csv contains similar information but without the survival status. Participants build a machine learning model using patterns in the training data to predict survival outcomes in the test data. Submissions must be in CSV format, containing exactly 418 rows and two columns: PassengerId and Survived (binary predictions: 1 - survived, 0 - deceased). The evaluation metric is accuracy, calculated as the percentage of correct predictions.\", additional_kwargs={}, response_metadata={}, id='270d464e-8166-4d8b-82d2-089b0510ef75'),\n",
       "  HumanMessage(content='## **Путешествие по морям: Расшифровка модели выживания на \"Титанике\"**\\n\\n### **Обзор**\\n\\n*   **Проблема:** Мы отправляемся в плавание по данным \"Титаника\", стремясь предсказать, кто выжил в трагическом крушении, а кто нет, основываясь на доступной информации о пассажирах. Задача Kaggle \"Titanic - Машинное обучение после катастрофы\" предоставляет данные о пассажирах и их статус выживания (или его отсутствие), чтобы помочь модели ИИ выявить характерные признаки. Цель состоит в том, чтобы заполнить отсутствующие статусы выживания для отдельной группы пассажиров, предоставив точный список `PassengerId` и их прогнозируемый результат `Survived` (1 для выживших, 0 для погибших).\\n*   **Цель:** Нашей миссией было разработать **модель AutoML** — по сути, эксперта по ИИ, строящего другой ИИ, — которая могла бы просматривать предоставленные данные, изучать закономерности и точно предсказывать, выжил ли пассажир в крушении \"Титаника\". Успех нашей модели измеряется её **точностью**: как часто её прогнозы совпадают с фактическими результатами.\\n\\n### **Предварительная обработка данных**\\n\\nПрежде чем наша интеллектуальная модель смогла начать обучение, необработанные, часто беспорядочные данные о пассажирах нуждались в тщательной очистке и стандартизации. Представьте себе, что это похоже на организацию хаотичной библиотеки, прежде чем вы сможете найти какие-либо книги.\\n\\n*   **Удаление несущественного:** Мы начали с удаления информации, которая была либо нерелевантной, либо потенциально запутывающей для модели. Это включало уникальные идентификаторы, такие как `PassengerId`, личные данные, такие как `Name`, сложные номера `Ticket` и фрагментарные данные `Cabin`. Сосредоточившись на релевантных признаках, модель может обучаться более эффективно.\\n*   **Заполнение пропусков (импутация):** Наборы данных часто содержат пробелы, где информация отсутствует. Вместо того чтобы отбрасывать эти записи, мы стратегически заполняли их.\\n    *   Для числовых значений, таких как `Age` или `Fare`, отсутствующие записи были заменены с использованием **средней импутации** для обеспечения единообразия. Например, в столбце `Age`, если возраст пассажира был неизвестен, мы заменяли его средним возрастом, рассчитанным по всем остальным пассажирам.\\n    *   Для категориальной информации, такой как порт `Embarked` (где пассажир сел на борт), отсутствующие записи были заменены **наиболее часто встречающимся** портом, гарантируя, что у каждого пассажира было место посадки.\\n*   **Преобразование категорий в понятный вид (One-Hot Encoding):** Модели машинного обучения, как правило, предпочитают числовые данные. Признаки, такие как `Sex` (мужчина/женщина) или `Embarked` (Саутгемптон/Шербур/Квинстаун), являются категориями. Мы преобразовали их в числовой формат с использованием \"однократного кодирования\" (one-hot encoding). Этот процесс создает новые столбцы для каждой категории (например, `Sex_male`, `Sex_female`), где \\'1\\' означает, что пассажир принадлежит к этой категории, а \\'0\\' — что нет. Это позволяет модели численно интерпретировать категориальные различия.\\n\\n### **Обзор конвейера**\\n\\nНаша модель была создана с использованием **Fedot**, фреймворка **AutoML (Automated Machine Learning)**. Представьте себе эксперта-архитектора ИИ, который автоматически проектирует, строит и тонко настраивает наилучший возможный конвейер машинного обучения для данной задачи. Вместо ручного тестирования бесчисленных моделей и настроек, Fedot интеллектуально исследует обширное пространство решений, чтобы найти оптимальное.\\n\\nАрхитектор ИИ рассмотрел и интегрировал несколько мощных \"строительных блоков\" в свою окончательную прогностическую систему:\\n\\n*   **`scaling` (масштабирование)**: Это важный шаг подготовки данных, часто применяемый к числовым данным.\\n    *   **Объяснение:** Нормализация гарантирует, что все признаки находятся в одном масштабе, улучшая производительность модели. Например, процесс масштабирования преобразует значения, такие как \"возраст\" (5-90), в 0-1. Это предотвращает несправедливое доминирование признаков с более крупными числовыми диапазонами (например, `Fare`) в процессе обучения по сравнению с признаками с меньшими диапазонами (например, `Age`).\\n*   **`logit` (Логистическая регрессия):** Фундаментальная статистическая модель, которая вычисляет вероятность определенного исхода (например, выживания), находя наилучшую линейную зависимость между признаками пассажира и его статусом выживания. Это похоже на проведение наиболее эффективной прямой линии для разделения \"выживших\" от \"не выживших\".\\n*   **`catboost`, `xgboost`, `lgbm` (Градиентные бустинговые машины):** Это современные ансамблевые модели обучения. Представьте их как высокоэффективные команды из множества небольших, \"слабых\" лиц, принимающих решения (вроде мини-блок-схем), работающих вместе. Каждый новый принимающий решения в команде учится на ошибках предыдущих, коллективно строя гораздо более сильную и точную систему прогнозирования.\\n    *   **Ключевые параметры:**\\n\\n        | Модель | Параметры | Объяснение |\\n        |---|---|---|\\n        | CatBoost | `num_trees: 3000, learning_rate: 0.03, max_depth: 5, l2_leaf_reg: 0.01` | CatBoost был выбран системой AutoML за его надёжную производительность. Эти параметры управляют его обучением: `num_trees` указывает, сколько мини-моделей объединяется, `learning_rate` определяет размер шага для улучшения, `max_depth` ограничивает сложность отдельных мини-моделей, а `l2_leaf_reg` помогает предотвратить слишком сильное запоминание моделью обучающих данных (переобучение). |\\n        | XGBoost | `early_stopping_rounds: 30` | Для XGBoost этот параметр указывает модели остановить обучение досрочно, если её производительность на валидационном наборе не улучшается в течение 30 последовательных раундов, экономя время и предотвращая переобучение. |\\n        | LightGBM | `early_stopping_rounds: 30`, `enable_categorical: True` | LightGBM, подобно XGBoost, использует `early_stopping_rounds: 30` для эффективной остановки обучения при стабилизации производительности, предотвращая излишние вычисления и переобучение. `enable_categorical: True` позволяет ему нативно обрабатывать категориальные признаки, оптимизируя процесс подготовки данных. |\\n\\n### **Основные фрагменты кода**\\n\\nДавайте заглянем под капот, чтобы увидеть некоторые основные операции в коде.\\n\\n1.  **Предварительная обработка данных (Ключевые фрагменты):**\\n    Эта часть кода определяет, как сырые данные очищаются и подготавливаются. Мы удаляем столбцы, не требующиеся для прогнозирования, заполняем любую отсутствующую информацию и преобразуем категориальные (текстовые) данные в числовые форматы, которые может понять наша модель ИИ.\\n\\n    ```python\\n    def transform_data(dataset: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\\n        # ... (code for dropping columns) ...\\n        cols_to_drop = [\\'PassengerId\\', \\'Name\\', \\'Ticket\\', \\'Cabin\\']\\n        for col in cols_to_drop:\\n            if col in features.columns:\\n                features = features.drop(columns=[col])\\n\\n        # Imputing missing values - \\'mean\\' strategy for numeric columns\\n        numeric_cols = features.select_dtypes(include=[np.number]).columns\\n        if len(numeric_cols) > 0:\\n            numeric_imputer = SimpleImputer(strategy=\\'mean\\')\\n            features[numeric_cols] = numeric_imputer.fit_transform(features[numeric_cols])\\n\\n        # Impute categorical columns (Sex, Embarked) before one-hot encoding\\n        object_cols_for_encoding = []\\n        if \\'Sex\\' in features.columns and features[\\'Sex\\'].dtype == \\'object\\':\\n            object_cols_for_encoding.append(\\'Sex\\')\\n        if \\'Embarked\\' in features.columns and features[\\'Embarked\\'].dtype == \\'object\\':\\n            object_cols_for_encoding.append(\\'Embarked\\')\\n        if len(object_cols_for_encoding) > 0:\\n            categorical_imputer = SimpleImputer(strategy=\\'most_frequent\\')\\n            for col in object_cols_for_encoding:\\n                if col in features.columns:\\n                    features[col] = categorical_imputer.fit_transform(features[[col]]).flatten()\\n\\n        # One-hot encode the specified categorical features\\n        if len(object_cols_for_encoding) > 0:\\n            features = pd.get_dummies(features, columns=object_cols_for_encoding, drop_first=False)\\n\\n        return features.values, target\\n    ```\\n    *Интерпретация:* Эта функция `transform_data` систематически подготавливает набор данных. Сначала она удаляет столбцы, определенные как бесполезные для прогнозирования. Затем она использует `SimpleImputer` для заполнения числовых пропусков (например, в `Age`) средним значением и категориальных пропусков (например, в `Embarked`) наиболее часто встречающимся значением. Наконец, `pd.get_dummies` преобразует текстовые категории, такие как `Sex` и `Embarked`, в числовые столбцы \"переключателей\" (0 или 1), делая их удобоваримыми для алгоритмов машинного обучения.\\n\\n2.  **Обучение, оценка и прогнозирование модели:**\\n    Этот раздел организует основной рабочий процесс машинного обучения: обучение модели AutoML, оценку её производительности на части обучающих данных, которые она ещё не видела, а затем использование полностью обученной модели для окончательных прогнозов на невидимых тестовых данных.\\n\\n    ```python\\n    def train_model(train_features: np.ndarray, train_target: np.ndarray):\\n        input_data = InputData.from_numpy(train_features, train_target, task=Task(TaskTypesEnum.classification))\\n        model = Fedot(problem=TaskTypesEnum.classification.value,\\n                timeout=1.0, # AutoML search will run for 1 second\\n                seed=42,\\n                cv_folds=5,\\n                preset=\\'best_quality\\', # Aim for the best possible model quality\\n                metric=\\'accuracy\\',\\n                n_jobs=1,\\n                with_tuning=True, # Allow AutoML to fine-tune model parameters\\n                show_progress=True)\\n\\n        model.fit(features=input_data) # This is where the AutoML magic happens: training the model\\n        return model\\n\\n    def evaluate_model(model, test_features: np.ndarray, test_target: np.ndarray):\\n        input_data = InputData.from_numpy(test_features, test_target, task=Task(TaskTypesEnum.classification))\\n        y_pred = model.predict(features=input_data)\\n        print(\"Model metrics: \", model.get_metrics())\\n        return model.get_metrics()\\n\\n    def automl_predict(model, features: np.ndarray) -> np.ndarray:\\n        input_data = InputData.from_numpy(features, None, task=Task(TaskTypesEnum.classification))\\n        predictions = model.predict(features=input_data)\\n        return predictions\\n    ```\\n    *Интерпретация:* `train_model` инициализирует `Fedot`, настраивая его для решения задачи `классификации` (выжил/не выжил). Он стремится получить модель `best_quality` (наилучшего качества) в течение 1 секунды автоматического поиска, измеряя успех по `accuracy` (точности). Вызов `model.fit()` — это место, где Fedot автоматически находит наилучшую комбинацию обработки данных и алгоритмов машинного обучения. Затем `evaluate_model` проверяет, насколько хорошо обученная модель работает на отдельном наборе данных, а `automl_predict` генерирует окончательные прогнозы выживания для невидимого тестового набора Kaggle.\\n\\n3.  **Создание файла для отправки:**\\n    После генерации прогнозов этот сегмент кода форматирует их в точности так, как требуется для соревнования Kaggle: две колонки, `PassengerId` и `Survived`, сохраняемые в CSV-файл.\\n\\n    ```python\\n    def create_model():\\n        # ... (data loading and splitting) ...\\n        test_passenger_ids = X_test_df[\\'PassengerId\\'] # Store IDs before dropping\\n\\n        # ... (model training, evaluation, and prediction calls) ...\\n        predictions:np.ndarray = automl_predict(model, test_features)\\n\\n        # Flatten predictions if they are in (N, 1) format, which is common for binary classification\\n        if predictions.ndim > 1 and predictions.shape[1] == 1:\\n            predictions = predictions.flatten()\\n        # Convert predictions (likely probabilities or raw scores) to binary class labels (0 or 1)\\n        predictions = (predictions >= 0.5).astype(int) # Convert probabilities to 0s or 1s\\n\\n        # Create the submission DataFrame\\n        output = pd.DataFrame({\\n            \\'PassengerId\\': test_passenger_ids,\\n            \\'Survived\\': predictions\\n        })\\n        output[\\'Survived\\'] = output[\\'Survived\\'].astype(int) # Ensure integer type\\n        output.to_csv(SUBMISSION_PATH, index=False) # Save to CSV\\n        return model_performance\\n    ```\\n    *Интерпретация:* Эта важная часть принимает необработанные числовые прогнозы (которые могут быть вероятностями) и преобразует их в бинарные (0 или 1) результаты выживания, устанавливая порог (например, вероятности 0.5 или выше становятся \\'survived\\' (выжившие)). Затем она объединяет эти бинарные прогнозы с исходным `PassengerId` в аккуратную таблицу. Наконец, `output.to_csv()` экспортирует эту таблицу в файл `.csv`, идеально отформатированный для отправки на соревнование Kaggle.\\n\\n### **Метрики**\\n\\nПосле всей обработки данных и построения модели, как показал себя наш ИИ?\\n\\n*   **Точность (Accuracy): `0.788`**\\n    *   **Объяснение:** Точность (Accuracy) показывает, как часто модель даёт правильные ответы. Точность в `0.788` означает, что наша модель правильно предсказала статус выживания примерно для **78.8%** пассажиров в оценочном наборе данных. Это надёжная производительность, указывающая на то, что модель уловила значимые закономерности в данных.\\n\\n### **Выводы**\\n\\nЭта модель AutoML, работающая на фреймворке **Fedot**, продемонстрировала высокую способность решать классическую задачу прогнозирования выживаемости на \"Титанике\". С **точностью приблизительно 78.8%**, модель эффективно использовала различные атрибуты пассажиров, чтобы различить тех, кто выжил, и тех, кто погиб. Этот проект подчеркивает мощь автоматизированного машинного обучения в быстром и эффективном построении надёжных прогностических моделей для сложных реальных задач классификации, превращая необработанные данные в значимые выводы с высокой степенью корректности.', additional_kwargs={}, response_metadata={}, name='AutoMLAgent', id='1c8ae38b-3e73-4bed-95b0-1e1662a00da0')]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import shutil\n",
    "output_path = os.path.join(os.getcwd(), 'output')\n",
    "if os.path.exists(output_path):\n",
    "    shutil.rmtree(output_path)\n",
    "os.makedirs(output_path, exist_ok=True)\n",
    "\n",
    "inference = AIInference()\n",
    "    \n",
    "fedot_ai = FedotAI(\n",
    "        task_path=dataset_path,\n",
    "        inference=inference,\n",
    "        workspace=output_path,\n",
    "        handlers=JupyterOutput().subscribe\n",
    "    )\n",
    "response = await fedot_ai.ainvoke(message=description)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "## **Путешествие по морям: Расшифровка модели выживания на \"Титанике\"**\n",
       "\n",
       "### **Обзор**\n",
       "\n",
       "*   **Проблема:** Мы отправляемся в плавание по данным \"Титаника\", стремясь предсказать, кто выжил в трагическом крушении, а кто нет, основываясь на доступной информации о пассажирах. Задача Kaggle \"Titanic - Машинное обучение после катастрофы\" предоставляет данные о пассажирах и их статус выживания (или его отсутствие), чтобы помочь модели ИИ выявить характерные признаки. Цель состоит в том, чтобы заполнить отсутствующие статусы выживания для отдельной группы пассажиров, предоставив точный список `PassengerId` и их прогнозируемый результат `Survived` (1 для выживших, 0 для погибших).\n",
       "*   **Цель:** Нашей миссией было разработать **модель AutoML** — по сути, эксперта по ИИ, строящего другой ИИ, — которая могла бы просматривать предоставленные данные, изучать закономерности и точно предсказывать, выжил ли пассажир в крушении \"Титаника\". Успех нашей модели измеряется её **точностью**: как часто её прогнозы совпадают с фактическими результатами.\n",
       "\n",
       "### **Предварительная обработка данных**\n",
       "\n",
       "Прежде чем наша интеллектуальная модель смогла начать обучение, необработанные, часто беспорядочные данные о пассажирах нуждались в тщательной очистке и стандартизации. Представьте себе, что это похоже на организацию хаотичной библиотеки, прежде чем вы сможете найти какие-либо книги.\n",
       "\n",
       "*   **Удаление несущественного:** Мы начали с удаления информации, которая была либо нерелевантной, либо потенциально запутывающей для модели. Это включало уникальные идентификаторы, такие как `PassengerId`, личные данные, такие как `Name`, сложные номера `Ticket` и фрагментарные данные `Cabin`. Сосредоточившись на релевантных признаках, модель может обучаться более эффективно.\n",
       "*   **Заполнение пропусков (импутация):** Наборы данных часто содержат пробелы, где информация отсутствует. Вместо того чтобы отбрасывать эти записи, мы стратегически заполняли их.\n",
       "    *   Для числовых значений, таких как `Age` или `Fare`, отсутствующие записи были заменены с использованием **средней импутации** для обеспечения единообразия. Например, в столбце `Age`, если возраст пассажира был неизвестен, мы заменяли его средним возрастом, рассчитанным по всем остальным пассажирам.\n",
       "    *   Для категориальной информации, такой как порт `Embarked` (где пассажир сел на борт), отсутствующие записи были заменены **наиболее часто встречающимся** портом, гарантируя, что у каждого пассажира было место посадки.\n",
       "*   **Преобразование категорий в понятный вид (One-Hot Encoding):** Модели машинного обучения, как правило, предпочитают числовые данные. Признаки, такие как `Sex` (мужчина/женщина) или `Embarked` (Саутгемптон/Шербур/Квинстаун), являются категориями. Мы преобразовали их в числовой формат с использованием \"однократного кодирования\" (one-hot encoding). Этот процесс создает новые столбцы для каждой категории (например, `Sex_male`, `Sex_female`), где '1' означает, что пассажир принадлежит к этой категории, а '0' — что нет. Это позволяет модели численно интерпретировать категориальные различия.\n",
       "\n",
       "### **Обзор конвейера**\n",
       "\n",
       "Наша модель была создана с использованием **Fedot**, фреймворка **AutoML (Automated Machine Learning)**. Представьте себе эксперта-архитектора ИИ, который автоматически проектирует, строит и тонко настраивает наилучший возможный конвейер машинного обучения для данной задачи. Вместо ручного тестирования бесчисленных моделей и настроек, Fedot интеллектуально исследует обширное пространство решений, чтобы найти оптимальное.\n",
       "\n",
       "Архитектор ИИ рассмотрел и интегрировал несколько мощных \"строительных блоков\" в свою окончательную прогностическую систему:\n",
       "\n",
       "*   **`scaling` (масштабирование)**: Это важный шаг подготовки данных, часто применяемый к числовым данным.\n",
       "    *   **Объяснение:** Нормализация гарантирует, что все признаки находятся в одном масштабе, улучшая производительность модели. Например, процесс масштабирования преобразует значения, такие как \"возраст\" (5-90), в 0-1. Это предотвращает несправедливое доминирование признаков с более крупными числовыми диапазонами (например, `Fare`) в процессе обучения по сравнению с признаками с меньшими диапазонами (например, `Age`).\n",
       "*   **`logit` (Логистическая регрессия):** Фундаментальная статистическая модель, которая вычисляет вероятность определенного исхода (например, выживания), находя наилучшую линейную зависимость между признаками пассажира и его статусом выживания. Это похоже на проведение наиболее эффективной прямой линии для разделения \"выживших\" от \"не выживших\".\n",
       "*   **`catboost`, `xgboost`, `lgbm` (Градиентные бустинговые машины):** Это современные ансамблевые модели обучения. Представьте их как высокоэффективные команды из множества небольших, \"слабых\" лиц, принимающих решения (вроде мини-блок-схем), работающих вместе. Каждый новый принимающий решения в команде учится на ошибках предыдущих, коллективно строя гораздо более сильную и точную систему прогнозирования.\n",
       "    *   **Ключевые параметры:**\n",
       "\n",
       "        | Модель | Параметры | Объяснение |\n",
       "        |---|---|---|\n",
       "        | CatBoost | `num_trees: 3000, learning_rate: 0.03, max_depth: 5, l2_leaf_reg: 0.01` | CatBoost был выбран системой AutoML за его надёжную производительность. Эти параметры управляют его обучением: `num_trees` указывает, сколько мини-моделей объединяется, `learning_rate` определяет размер шага для улучшения, `max_depth` ограничивает сложность отдельных мини-моделей, а `l2_leaf_reg` помогает предотвратить слишком сильное запоминание моделью обучающих данных (переобучение). |\n",
       "        | XGBoost | `early_stopping_rounds: 30` | Для XGBoost этот параметр указывает модели остановить обучение досрочно, если её производительность на валидационном наборе не улучшается в течение 30 последовательных раундов, экономя время и предотвращая переобучение. |\n",
       "        | LightGBM | `early_stopping_rounds: 30`, `enable_categorical: True` | LightGBM, подобно XGBoost, использует `early_stopping_rounds: 30` для эффективной остановки обучения при стабилизации производительности, предотвращая излишние вычисления и переобучение. `enable_categorical: True` позволяет ему нативно обрабатывать категориальные признаки, оптимизируя процесс подготовки данных. |\n",
       "\n",
       "### **Основные фрагменты кода**\n",
       "\n",
       "Давайте заглянем под капот, чтобы увидеть некоторые основные операции в коде.\n",
       "\n",
       "1.  **Предварительная обработка данных (Ключевые фрагменты):**\n",
       "    Эта часть кода определяет, как сырые данные очищаются и подготавливаются. Мы удаляем столбцы, не требующиеся для прогнозирования, заполняем любую отсутствующую информацию и преобразуем категориальные (текстовые) данные в числовые форматы, которые может понять наша модель ИИ.\n",
       "\n",
       "    ```python\n",
       "    def transform_data(dataset: pd.DataFrame) -> Tuple[np.ndarray, np.ndarray]:\n",
       "        # ... (code for dropping columns) ...\n",
       "        cols_to_drop = ['PassengerId', 'Name', 'Ticket', 'Cabin']\n",
       "        for col in cols_to_drop:\n",
       "            if col in features.columns:\n",
       "                features = features.drop(columns=[col])\n",
       "\n",
       "        # Imputing missing values - 'mean' strategy for numeric columns\n",
       "        numeric_cols = features.select_dtypes(include=[np.number]).columns\n",
       "        if len(numeric_cols) > 0:\n",
       "            numeric_imputer = SimpleImputer(strategy='mean')\n",
       "            features[numeric_cols] = numeric_imputer.fit_transform(features[numeric_cols])\n",
       "\n",
       "        # Impute categorical columns (Sex, Embarked) before one-hot encoding\n",
       "        object_cols_for_encoding = []\n",
       "        if 'Sex' in features.columns and features['Sex'].dtype == 'object':\n",
       "            object_cols_for_encoding.append('Sex')\n",
       "        if 'Embarked' in features.columns and features['Embarked'].dtype == 'object':\n",
       "            object_cols_for_encoding.append('Embarked')\n",
       "        if len(object_cols_for_encoding) > 0:\n",
       "            categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
       "            for col in object_cols_for_encoding:\n",
       "                if col in features.columns:\n",
       "                    features[col] = categorical_imputer.fit_transform(features[[col]]).flatten()\n",
       "\n",
       "        # One-hot encode the specified categorical features\n",
       "        if len(object_cols_for_encoding) > 0:\n",
       "            features = pd.get_dummies(features, columns=object_cols_for_encoding, drop_first=False)\n",
       "\n",
       "        return features.values, target\n",
       "    ```\n",
       "    *Интерпретация:* Эта функция `transform_data` систематически подготавливает набор данных. Сначала она удаляет столбцы, определенные как бесполезные для прогнозирования. Затем она использует `SimpleImputer` для заполнения числовых пропусков (например, в `Age`) средним значением и категориальных пропусков (например, в `Embarked`) наиболее часто встречающимся значением. Наконец, `pd.get_dummies` преобразует текстовые категории, такие как `Sex` и `Embarked`, в числовые столбцы \"переключателей\" (0 или 1), делая их удобоваримыми для алгоритмов машинного обучения.\n",
       "\n",
       "2.  **Обучение, оценка и прогнозирование модели:**\n",
       "    Этот раздел организует основной рабочий процесс машинного обучения: обучение модели AutoML, оценку её производительности на части обучающих данных, которые она ещё не видела, а затем использование полностью обученной модели для окончательных прогнозов на невидимых тестовых данных.\n",
       "\n",
       "    ```python\n",
       "    def train_model(train_features: np.ndarray, train_target: np.ndarray):\n",
       "        input_data = InputData.from_numpy(train_features, train_target, task=Task(TaskTypesEnum.classification))\n",
       "        model = Fedot(problem=TaskTypesEnum.classification.value,\n",
       "                timeout=1.0, # AutoML search will run for 1 second\n",
       "                seed=42,\n",
       "                cv_folds=5,\n",
       "                preset='best_quality', # Aim for the best possible model quality\n",
       "                metric='accuracy',\n",
       "                n_jobs=1,\n",
       "                with_tuning=True, # Allow AutoML to fine-tune model parameters\n",
       "                show_progress=True)\n",
       "\n",
       "        model.fit(features=input_data) # This is where the AutoML magic happens: training the model\n",
       "        return model\n",
       "\n",
       "    def evaluate_model(model, test_features: np.ndarray, test_target: np.ndarray):\n",
       "        input_data = InputData.from_numpy(test_features, test_target, task=Task(TaskTypesEnum.classification))\n",
       "        y_pred = model.predict(features=input_data)\n",
       "        print(\"Model metrics: \", model.get_metrics())\n",
       "        return model.get_metrics()\n",
       "\n",
       "    def automl_predict(model, features: np.ndarray) -> np.ndarray:\n",
       "        input_data = InputData.from_numpy(features, None, task=Task(TaskTypesEnum.classification))\n",
       "        predictions = model.predict(features=input_data)\n",
       "        return predictions\n",
       "    ```\n",
       "    *Интерпретация:* `train_model` инициализирует `Fedot`, настраивая его для решения задачи `классификации` (выжил/не выжил). Он стремится получить модель `best_quality` (наилучшего качества) в течение 1 секунды автоматического поиска, измеряя успех по `accuracy` (точности). Вызов `model.fit()` — это место, где Fedot автоматически находит наилучшую комбинацию обработки данных и алгоритмов машинного обучения. Затем `evaluate_model` проверяет, насколько хорошо обученная модель работает на отдельном наборе данных, а `automl_predict` генерирует окончательные прогнозы выживания для невидимого тестового набора Kaggle.\n",
       "\n",
       "3.  **Создание файла для отправки:**\n",
       "    После генерации прогнозов этот сегмент кода форматирует их в точности так, как требуется для соревнования Kaggle: две колонки, `PassengerId` и `Survived`, сохраняемые в CSV-файл.\n",
       "\n",
       "    ```python\n",
       "    def create_model():\n",
       "        # ... (data loading and splitting) ...\n",
       "        test_passenger_ids = X_test_df['PassengerId'] # Store IDs before dropping\n",
       "\n",
       "        # ... (model training, evaluation, and prediction calls) ...\n",
       "        predictions:np.ndarray = automl_predict(model, test_features)\n",
       "\n",
       "        # Flatten predictions if they are in (N, 1) format, which is common for binary classification\n",
       "        if predictions.ndim > 1 and predictions.shape[1] == 1:\n",
       "            predictions = predictions.flatten()\n",
       "        # Convert predictions (likely probabilities or raw scores) to binary class labels (0 or 1)\n",
       "        predictions = (predictions >= 0.5).astype(int) # Convert probabilities to 0s or 1s\n",
       "\n",
       "        # Create the submission DataFrame\n",
       "        output = pd.DataFrame({\n",
       "            'PassengerId': test_passenger_ids,\n",
       "            'Survived': predictions\n",
       "        })\n",
       "        output['Survived'] = output['Survived'].astype(int) # Ensure integer type\n",
       "        output.to_csv(SUBMISSION_PATH, index=False) # Save to CSV\n",
       "        return model_performance\n",
       "    ```\n",
       "    *Интерпретация:* Эта важная часть принимает необработанные числовые прогнозы (которые могут быть вероятностями) и преобразует их в бинарные (0 или 1) результаты выживания, устанавливая порог (например, вероятности 0.5 или выше становятся 'survived' (выжившие)). Затем она объединяет эти бинарные прогнозы с исходным `PassengerId` в аккуратную таблицу. Наконец, `output.to_csv()` экспортирует эту таблицу в файл `.csv`, идеально отформатированный для отправки на соревнование Kaggle.\n",
       "\n",
       "### **Метрики**\n",
       "\n",
       "После всей обработки данных и построения модели, как показал себя наш ИИ?\n",
       "\n",
       "*   **Точность (Accuracy): `0.788`**\n",
       "    *   **Объяснение:** Точность (Accuracy) показывает, как часто модель даёт правильные ответы. Точность в `0.788` означает, что наша модель правильно предсказала статус выживания примерно для **78.8%** пассажиров в оценочном наборе данных. Это надёжная производительность, указывающая на то, что модель уловила значимые закономерности в данных.\n",
       "\n",
       "### **Выводы**\n",
       "\n",
       "Эта модель AutoML, работающая на фреймворке **Fedot**, продемонстрировала высокую способность решать классическую задачу прогнозирования выживаемости на \"Титанике\". С **точностью приблизительно 78.8%**, модель эффективно использовала различные атрибуты пассажиров, чтобы различить тех, кто выжил, и тех, кто погиб. Этот проект подчеркивает мощь автоматизированного машинного обучения в быстром и эффективном построении надёжных прогностических моделей для сложных реальных задач классификации, превращая необработанные данные в значимые выводы с высокой степенью корректности."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "Markdown(response['messages'][-1].content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
