{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feb4ac62",
   "metadata": {},
   "source": [
    "# TranslatorAgent Interactive Example\n",
    "\n",
    "This notebook demonstrates how to use the `TranslatorAgent` class for AI-powered text translation with automatic language detection and formatting preservation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbcfeb9b",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "First, let's import the necessary modules and set up the translator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8e2d745d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TranslatorAgent initialized successfully!\n"
     ]
    }
   ],
   "source": [
    "import sys, os\n",
    "\n",
    "module_path = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "\n",
    "if module_path not in sys.path:\n",
    "    sys.path.insert(0, module_path)\n",
    "    \n",
    "from fedotllm.agents.translator import TranslatorAgent\n",
    "from fedotllm.llm import AIInference\n",
    "\n",
    "# Initialize AI inference\n",
    "# Make sure to set FEDOTLLM_LLM_API_KEY environment variable\n",
    "inference = AIInference()\n",
    "\n",
    "# Create translator instance\n",
    "translator = TranslatorAgent(inference)\n",
    "\n",
    "print(\"TranslatorAgent initialized successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b73226d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "report = \"\"\"\n",
    "# üìù Multi-Class Prediction of Obesity Risk: Tackling Health Through Machine Learning  \n",
    "\n",
    "## üöÄ Overview  \n",
    "This competition, **Playground Series - Season 4, Episode 2**, challenges participants to predict obesity risk in individuals using machine learning. The dataset includes factors like age, physical activity, and dietary habits. The aim is multi-class classification of the target variable `NObeyesdad`, which represents five distinct weight categories ranging from underweight to obesity. With cardiovascular disease being a major global concern, this task holds practical significance in public health.  \n",
    "\n",
    "## üîß Data Preprocessing  \n",
    "Preparing the dataset is a critical step before feeding it to the model. Here's what we did:  \n",
    "- **Imputation of Missing Values:** Addressed any gaps in the data by:  \n",
    "  - For numerical features, replacing missing values with the column mean. Example: A missing age value is substituted with the average age of the dataset.  \n",
    "  - For categorical features, filling missing entries with the most frequent category.  \n",
    "- **One-Hot Encoding:** Converted categorical variables (e.g., gender) into numerical representations for compatibility with machine learning algorithms. Example: Female ‚Üí [1, 0], Male ‚Üí [0, 1].  \n",
    "- **Normalization (Scaling):** Ensured all numerical features were on a similar scale (e.g., 0 to 1). This prevents variables with large ranges (e.g., income) from dominating the model training.  \n",
    "\n",
    "Here‚Äôs what the preprocessing looked like in code:  \n",
    "```python\n",
    "# Impute numerical features\n",
    "numeric_imputer = SimpleImputer(strategy='mean')\n",
    "features[numeric_cols] = numeric_imputer.fit_transform(features[numeric_cols])\n",
    "\n",
    "# Impute categorical features\n",
    "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
    "features[categorical_cols] = categorical_imputer.fit_transform(features[categorical_cols])\n",
    "\n",
    "# One-hot encoding\n",
    "features = pd.get_dummies(features, columns=categorical_cols, drop_first=True)\n",
    "\n",
    "# Normalize features (0-1 range)\n",
    "scaler = MinMaxScaler()\n",
    "features = scaler.fit_transform(features)\n",
    "```  \n",
    "\n",
    "## üîÑ Pipeline Summary  \n",
    "The model pipeline included pre-tuned AutoML algorithms to enhance prediction without manually optimizing hyperparameters. Here's the pipeline overview:  \n",
    "1. **Data Preprocessing:** Handled missing data, scaling, and one-hot encoding.  \n",
    "2. **Model Aggregation:** Leveraged a stack of leading machine learning models:  \n",
    "   - `CatBoost` for handling categorical data effectively.  \n",
    "   - `XGBoost` and `LightGBM` for fast, scalable gradient boosting.  \n",
    "   - A logistic regression model for baseline linear performance.  \n",
    "3. **Model Selection:** AutoML dynamically selected the best model configuration during cross-validation.  \n",
    "\n",
    "### Key Pipeline Parameters  \n",
    "| Model        | Key Parameters                                       | Explanation                                                          |  \n",
    "|--------------|------------------------------------------------------|----------------------------------------------------------------------|  \n",
    "| **CatBoost** | num_trees=3000, learning_rate=0.03, max_depth=5      | Efficient with categorical and tabular datasets.                     |  \n",
    "| **XGBoost**  | booster='gbtree', early_stopping_rounds=30           | Popular for its robustness and high predictive power.                |  \n",
    "| **LightGBM** | bagging_fraction=0.85, max_depth=-1                  | Designed for speed and scalability with large datasets.              |  \n",
    "| **Scaling**  | MinMaxScaler for normalization                       | Helps gradient-based models converge faster and prevent dominance.   |  \n",
    "\n",
    "## üñ•Ô∏è Code Highlights  \n",
    "Here‚Äôs a breakdown of the most critical sections of the code:\n",
    "\n",
    "1. **Data Preprocessing**  \n",
    "   Converts the raw dataset to a clean format usable by the models.  \n",
    "```python\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# Impute missing values and scale features\n",
    "def transform_data(dataset: pd.DataFrame):\n",
    "    features = dataset.drop(columns=['NObeyesdad', 'id'])\n",
    "    target = dataset['NObeyesdad'].values if 'NObeyesdad' in dataset.columns else None\n",
    "\n",
    "    numeric_imputer = SimpleImputer(strategy='mean')\n",
    "    features = numeric_imputer.fit_transform(features)\n",
    "\n",
    "    scaler = MinMaxScaler()\n",
    "    features = scaler.fit_transform(features)\n",
    "\n",
    "    return features, target\n",
    "```  \n",
    "\n",
    "2. **Model Training and AutoML**  \n",
    "   Automated the model training process using the **FEDOT AutoML framework** to efficiently explore high-quality predictive models.  \n",
    "```python\n",
    "from fedot.api.main import Fedot\n",
    "from fedot.core.repository.tasks import Task, TaskTypesEnum\n",
    "\n",
    "def train_model(features, target):\n",
    "    # Configure and train the model\n",
    "    pipeline = Fedot(problem='classification', preset='best_quality', timeout=1.0, metric='accuracy')\n",
    "    pipeline.fit(features, target)\n",
    "    return pipeline\n",
    "```  \n",
    "\n",
    "3. **Evaluation and Predictions**  \n",
    "   Evaluated model performance and prepared submission files.  \n",
    "```python\n",
    "def create_submission():\n",
    "    predictions = model.predict(test_features)\n",
    "    submission_df = pd.DataFrame({'id': test_data['id'], 'NObeyesdad': predictions})\n",
    "    submission_df.to_csv(\"submission.csv\", index=False)\n",
    "```  \n",
    "\n",
    "## üìä Metrics  \n",
    "\n",
    "The model's performance is evaluated using **accuracy**, which measures the proportion of correct predictions among the total predictions:  \n",
    "- Accuracy: 90.7%  \n",
    "This means the model correctly identifies the obesity risk category for 9 out of every 10 individuals in the evaluation set.  \n",
    "\n",
    "### Interpretation:  \n",
    "- **Strengths:** High accuracy demonstrates strong predictive capability across multiple classes.  \n",
    "- **Potential Challenges:** Class imbalance (some obesity categories may have fewer instances) could affect predictions for minority classes.  \n",
    "\n",
    "## üí° Takeaways  \n",
    "\n",
    "This model effectively predicts obesity risk with **90.7% accuracy**, a promising result for public health applications. By leveraging advanced AutoML techniques and robust preprocessing, it demonstrates a scalable, efficient approach to tackle similar classification problems. While this is a synthetic competition dataset, the pipeline could easily be adapted for real-world use cases like predicting cardiovascular risk or targeting dietary interventions.  \n",
    "\n",
    "Modeling obesity risk is not just about prediction‚Äîit's about enabling preventive healthcare measures that could save lives. This competition shows how machine learning can make serious strides in addressing global health challenges.\n",
    "\"\"\"\n",
    "\n",
    "# üìù –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–µ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –†–∏—Å–∫–∞ –û–∂–∏—Ä–µ–Ω–∏—è\n",
    "## üöÄ –û–±–∑–æ—Ä  \n",
    "–≠—Ç–æ—Ç –∫–æ–Ω–∫—É—Ä—Å, **Playground Series - Season 4, Episode 2**, —Å—Ç–∞–≤–∏—Ç –ø–µ—Ä–µ–¥ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –∑–∞–¥–∞—á—É...\n",
    "## üîß –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –î–∞–Ω–Ω—ã—Ö  \n",
    " –í–æ—Ç —á—Ç–æ –º—ã —Å–¥–µ–ª–∞–ª–∏:  \n",
    "- **–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:** –£—Å—Ç—Ä–∞–Ω–∏–ª–∏ –ª—é–±—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º: ...\n",
    "–í–æ—Ç –∫–∞–∫ –≤—ã–≥–ª—è–¥–µ–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∫–æ–¥–µ:...\n",
    "## üîÑ –û–±–∑–æ—Ä –ö–æ–Ω–≤–µ–π–µ—Ä–∞  \n",
    "–í–æ—Ç –æ–±–∑–æ—Ä –∫–æ–Ω–≤–µ–π–µ—Ä–∞:  \n",
    "1. **–ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å—Ç–µ–∫ –≤–µ–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:  \n",
    "   - `CatBoost` –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.\n",
    "...\n",
    "### –ö–ª—é—á–µ–≤—ã–µ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ö–æ–Ω–≤–µ–π–µ—Ä–∞  \n",
    "| –ú–æ–¥–µ–ª—å        | –ö–ª—é—á–µ–≤—ã–µ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã                                   | –û–±—ä—è—Å–Ω–µ–Ω–∏–µ                                                                      |  \n",
    "|--------------|------------------------------------------------------|----------------------------------------------------------------------------------|  \n",
    "| **CatBoost** | num_trees=3000, learning_rate=0.03, max_depth=5      | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö.                      |  \n",
    "...\n",
    "## üñ•Ô∏è –û—Å–Ω–æ–≤–Ω—ã–µ –ú–æ–º–µ–Ω—Ç—ã –ö–æ–¥–∞  \n",
    "## üìä –ú–µ—Ç—Ä–∏–∫–∏  \n",
    "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å... –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **—Ç–æ—á–Ω–æ—Å—Ç–∏ (accuracy)**...–¢–æ—á–Ω–æ—Å—Ç—å: 90.7% ...–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ...\n",
    "### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:  \n",
    "## üí° –í—ã–≤–æ–¥—ã\n",
    "–≠—Ç–∞ –º–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç —Ä–∏—Å–∫ –æ–∂–∏—Ä–µ–Ω–∏—è —Å **—Ç–æ—á–Ω–æ—Å—Ç—å—é 90.7%**, —á—Ç–æ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5eee1f33",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "# üìù –ú–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–æ–µ –ü—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ –†–∏—Å–∫–∞ –û–∂–∏—Ä–µ–Ω–∏—è: –†–µ—à–µ–Ω–∏–µ –ó–∞–¥–∞—á –ó–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Å –ü–æ–º–æ—â—å—é –ú–∞—à–∏–Ω–Ω–æ–≥–æ –û–±—É—á–µ–Ω–∏—è  \n",
       "\n",
       "## üöÄ –û–±–∑–æ—Ä  \n",
       "–≠—Ç–æ—Ç –∫–æ–Ω–∫—É—Ä—Å, **Playground Series - Season 4, Episode 2**, —Å—Ç–∞–≤–∏—Ç –ø–µ—Ä–µ–¥ —É—á–∞—Å—Ç–Ω–∏–∫–∞–º–∏ –∑–∞–¥–∞—á—É –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è —Ä–∏—Å–∫–∞ –æ–∂–∏—Ä–µ–Ω–∏—è —É –ª—é–¥–µ–π —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ù–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤–∫–ª—é—á–∞–µ—Ç —Ç–∞–∫–∏–µ —Ñ–∞–∫—Ç–æ—Ä—ã, –∫–∞–∫ –≤–æ–∑—Ä–∞—Å—Ç, —Ñ–∏–∑–∏—á–µ—Å–∫–∞—è –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç—å –∏ –ø–∏—â–µ–≤—ã–µ –ø—Ä–∏–≤—ã—á–∫–∏. –¶–µ–ª—å ‚Äî –º–Ω–æ–≥–æ–∫–ª–∞—Å—Å–æ–≤–∞—è –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏—è —Ü–µ–ª–µ–≤–æ–π –ø–µ—Ä–µ–º–µ–Ω–Ω–æ–π `NObeyesdad`, –∫–æ—Ç–æ—Ä–∞—è –ø—Ä–µ–¥—Å—Ç–∞–≤–ª—è–µ—Ç –ø—è—Ç—å —Ä–∞–∑–ª–∏—á–Ω—ã—Ö –∫–∞—Ç–µ–≥–æ—Ä–∏–π –≤–µ—Å–∞, –æ—Ç –Ω–µ–¥–æ—Å—Ç–∞—Ç–æ—á–Ω–æ–≥–æ –¥–æ –æ–∂–∏—Ä–µ–Ω–∏—è. –ü–æ—Å–∫–æ–ª—å–∫—É —Å–µ—Ä–¥–µ—á–Ω–æ-—Å–æ—Å—É–¥–∏—Å—Ç—ã–µ –∑–∞–±–æ–ª–µ–≤–∞–Ω–∏—è —è–≤–ª—è—é—Ç—Å—è —Å–µ—Ä—å–µ–∑–Ω–æ–π –≥–ª–æ–±–∞–ª—å–Ω–æ–π –ø—Ä–æ–±–ª–µ–º–æ–π, —ç—Ç–∞ –∑–∞–¥–∞—á–∞ –∏–º–µ–µ—Ç –ø—Ä–∞–∫—Ç–∏—á–µ—Å–∫–æ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –¥–ª—è –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è.  \n",
       "\n",
       "## üîß –ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –û–±—Ä–∞–±–æ—Ç–∫–∞ –î–∞–Ω–Ω—ã—Ö  \n",
       "–ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –Ω–∞–±–æ—Ä–∞ –¥–∞–Ω–Ω—ã—Ö ‚Äî –∫—Ä–∏—Ç–∏—á–µ—Å–∫–∏ –≤–∞–∂–Ω—ã–π —à–∞–≥ –ø–µ—Ä–µ–¥ –ø–µ—Ä–µ–¥–∞—á–µ–π –µ–≥–æ –º–æ–¥–µ–ª–∏. –í–æ—Ç —á—Ç–æ –º—ã —Å–¥–µ–ª–∞–ª–∏:  \n",
       "- **–ó–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π:** –£—Å—Ç—Ä–∞–Ω–∏–ª–∏ –ª—é–±—ã–µ –ø—Ä–æ–±–µ–ª—ã –≤ –¥–∞–Ω–Ω—ã—Ö –ø—É—Ç–µ–º:  \n",
       "  - –î–ª—è —á–∏—Å–ª–æ–≤—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Äî –∑–∞–º–µ–Ω–∞ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–Ω–∞—á–µ–Ω–∏–π —Å—Ä–µ–¥–Ω–∏–º –ø–æ —Å—Ç–æ–ª–±—Ü—É. –ü—Ä–∏–º–µ—Ä: –û—Ç—Å—É—Ç—Å—Ç–≤—É—é—â–µ–µ –∑–Ω–∞—á–µ–Ω–∏–µ –≤–æ–∑—Ä–∞—Å—Ç–∞ –∑–∞–º–µ–Ω—è–µ—Ç—Å—è —Å—Ä–µ–¥–Ω–∏–º –≤–æ–∑—Ä–∞—Å—Ç–æ–º –ø–æ –Ω–∞–±–æ—Ä—É –¥–∞–Ω–Ω—ã—Ö.  \n",
       "  - –î–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –ø—Ä–∏–∑–Ω–∞–∫–æ–≤ ‚Äî –∑–∞–ø–æ–ª–Ω–µ–Ω–∏–µ –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã—Ö –∑–∞–ø–∏—Å–µ–π –Ω–∞–∏–±–æ–ª–µ–µ —á–∞—Å—Ç–æ–π –∫–∞—Ç–µ–≥–æ—Ä–∏–µ–π.  \n",
       "- **One-Hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ (–ì–æ—Ä—è—á–µ–µ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ):** –ü—Ä–µ–æ–±—Ä–∞–∑–æ–≤–∞–ª–∏ –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –ø–æ–ª) –≤ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–µ–¥—Å—Ç–∞–≤–ª–µ–Ω–∏—è –¥–ª—è —Å–æ–≤–º–µ—Å—Ç–∏–º–æ—Å—Ç–∏ —Å –∞–ª–≥–æ—Ä–∏—Ç–º–∞–º–∏ –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è. –ü—Ä–∏–º–µ—Ä: –ñ–µ–Ω—Å–∫–∏–π ‚Üí [1, 0], –ú—É–∂—Å–∫–æ–π ‚Üí [0, 1].  \n",
       "- **–ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è (–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ):** –£–±–µ–¥–∏–ª–∏—Å—å, —á—Ç–æ –≤—Å–µ —á–∏—Å–ª–æ–≤—ã–µ –ø—Ä–∏–∑–Ω–∞–∫–∏ –Ω–∞—Ö–æ–¥—è—Ç—Å—è –≤ –∞–Ω–∞–ª–æ–≥–∏—á–Ω–æ–º –º–∞—Å—à—Ç–∞–±–µ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –æ—Ç 0 –¥–æ 1). –≠—Ç–æ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å –±–æ–ª—å—à–∏–º–∏ –¥–∏–∞–ø–∞–∑–æ–Ω–∞–º–∏ (–Ω–∞–ø—Ä–∏–º–µ—Ä, –¥–æ—Ö–æ–¥) –ø—Ä–∏ –æ–±—É—á–µ–Ω–∏–∏ –º–æ–¥–µ–ª–∏.  \n",
       "\n",
       "–í–æ—Ç –∫–∞–∫ –≤—ã–≥–ª—è–¥–µ–ª–∞ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –≤ –∫–æ–¥–µ:  \n",
       "```python\n",
       "# Impute numerical features\n",
       "numeric_imputer = SimpleImputer(strategy='mean')\n",
       "features[numeric_cols] = numeric_imputer.fit_transform(features[numeric_cols])\n",
       "\n",
       "# Impute categorical features\n",
       "categorical_imputer = SimpleImputer(strategy='most_frequent')\n",
       "features[categorical_cols] = categorical_imputer.fit_transform(features[categorical_cols])\n",
       "\n",
       "# One-hot encoding\n",
       "features = pd.get_dummies(features, columns=categorical_cols, drop_first=True)\n",
       "\n",
       "# Normalize features (0-1 range)\n",
       "scaler = MinMaxScaler()\n",
       "features = scaler.fit_transform(features)\n",
       "```  \n",
       "\n",
       "## üîÑ –û–±–∑–æ—Ä –ö–æ–Ω–≤–µ–π–µ—Ä–∞  \n",
       "–ö–æ–Ω–≤–µ–π–µ—Ä –º–æ–¥–µ–ª–∏ –≤–∫–ª—é—á–∞–ª –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –Ω–∞—Å—Ç—Ä–æ–µ–Ω–Ω—ã–µ –∞–ª–≥–æ—Ä–∏—Ç–º—ã AutoML –¥–ª—è —É–ª—É—á—à–µ–Ω–∏—è –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏—è –±–µ–∑ —Ä—É—á–Ω–æ–π –æ–ø—Ç–∏–º–∏–∑–∞—Ü–∏–∏ –≥–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä–æ–≤. –í–æ—Ç –æ–±–∑–æ—Ä –∫–æ–Ω–≤–µ–π–µ—Ä–∞:  \n",
       "1. **–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö:** –û–±—Ä–∞–±–æ—Ç–∞–Ω—ã –ø—Ä–æ–ø—É—â–µ–Ω–Ω—ã–µ –¥–∞–Ω–Ω—ã–µ, –º–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ –∏ one-hot –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ.  \n",
       "2. **–ê–≥—Ä–µ–≥–∞—Ü–∏—è –º–æ–¥–µ–ª–µ–π:** –ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω —Å—Ç–µ–∫ –≤–µ–¥—É—â–∏—Ö –º–æ–¥–µ–ª–µ–π –º–∞—à–∏–Ω–Ω–æ–≥–æ –æ–±—É—á–µ–Ω–∏—è:  \n",
       "   - `CatBoost` –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–π —Ä–∞–±–æ—Ç—ã —Å –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã–º–∏ –¥–∞–Ω–Ω—ã–º–∏.  \n",
       "   - `XGBoost` –∏ `LightGBM` –¥–ª—è –±—ã—Å—Ç—Ä–æ–≥–æ, –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ–≥–æ –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω–æ–≥–æ –±—É—Å—Ç–∏–Ω–≥–∞.  \n",
       "   - –ú–æ–¥–µ–ª—å –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫–æ–π —Ä–µ–≥—Ä–µ—Å—Å–∏–∏ –¥–ª—è –±–∞–∑–æ–≤–æ–π –ª–∏–Ω–µ–π–Ω–æ–π –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç–∏.  \n",
       "3. **–í—ã–±–æ—Ä –º–æ–¥–µ–ª–∏:** AutoML –¥–∏–Ω–∞–º–∏—á–µ—Å–∫–∏ –≤—ã–±–∏—Ä–∞–ª –ª—É—á—à—É—é –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é –º–æ–¥–µ–ª–∏ –≤–æ –≤—Ä–µ–º—è –∫—Ä–æ—Å—Å-–≤–∞–ª–∏–¥–∞—Ü–∏–∏.  \n",
       "\n",
       "### –ö–ª—é—á–µ–≤—ã–µ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ö–æ–Ω–≤–µ–π–µ—Ä–∞  \n",
       "| –ú–æ–¥–µ–ª—å        | –ö–ª—é—á–µ–≤—ã–µ –ü–∞—Ä–∞–º–µ—Ç—Ä—ã                                   | –û–±—ä—è—Å–Ω–µ–Ω–∏–µ                                                                      |  \n",
       "|--------------|------------------------------------------------------|----------------------------------------------------------------------------------|  \n",
       "| **CatBoost** | num_trees=3000, learning_rate=0.03, max_depth=5      | –≠—Ñ—Ñ–µ–∫—Ç–∏–≤–µ–Ω –¥–ª—è –∫–∞—Ç–µ–≥–æ—Ä–∏–∞–ª—å–Ω—ã—Ö –∏ —Ç–∞–±–ª–∏—á–Ω—ã—Ö –Ω–∞–±–æ—Ä–æ–≤ –¥–∞–Ω–Ω—ã—Ö.                      |  \n",
       "| **XGBoost**  | booster='gbtree', early_stopping_rounds=30           | –ü–æ–ø—É–ª—è—Ä–µ–Ω –±–ª–∞–≥–æ–¥–∞—Ä—è —Å–≤–æ–µ–π –Ω–∞–¥–µ–∂–Ω–æ—Å—Ç–∏ –∏ –≤—ã—Å–æ–∫–æ–π –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω–æ–π —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç–∏.    |  \n",
       "| **LightGBM** | bagging_fraction=0.85, max_depth=-1                  | –†–∞–∑—Ä–∞–±–æ—Ç–∞–Ω –¥–ª—è —Å–∫–æ—Ä–æ—Å—Ç–∏ –∏ –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º–æ—Å—Ç–∏ —Å –±–æ–ª—å—à–∏–º–∏ –Ω–∞–±–æ—Ä–∞–º–∏ –¥–∞–Ω–Ω—ã—Ö.          |  \n",
       "| **–ú–∞—Å—à—Ç–∞–±–∏—Ä–æ–≤–∞–Ω–∏–µ**  | MinMaxScaler –¥–ª—è –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏–∏                       | –ü–æ–º–æ–≥–∞–µ—Ç –≥—Ä–∞–¥–∏–µ–Ω—Ç–Ω—ã–º –º–æ–¥–µ–ª—è–º –±—ã—Å—Ç—Ä–µ–µ —Å—Ö–æ–¥–∏—Ç—å—Å—è –∏ –ø—Ä–µ–¥–æ—Ç–≤—Ä–∞—â–∞–µ—Ç –¥–æ–º–∏–Ω–∏—Ä–æ–≤–∞–Ω–∏–µ.   |  \n",
       "\n",
       "## üñ•Ô∏è –û—Å–Ω–æ–≤–Ω—ã–µ –ú–æ–º–µ–Ω—Ç—ã –ö–æ–¥–∞  \n",
       "–í–æ—Ç —Ä–∞–∑–±–∏–≤–∫–∞ –Ω–∞–∏–±–æ–ª–µ–µ –≤–∞–∂–Ω—ã—Ö —Ä–∞–∑–¥–µ–ª–æ–≤ –∫–æ–¥–∞:\n",
       "\n",
       "1. **–ü—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞ –¥–∞–Ω–Ω—ã—Ö**  \n",
       "   –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –∏—Å—Ö–æ–¥–Ω—ã–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –≤ —á–∏—Å—Ç—ã–π —Ñ–æ—Ä–º–∞—Ç, –ø—Ä–∏–≥–æ–¥–Ω—ã–π –¥–ª—è –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è –º–æ–¥–µ–ª—è–º–∏.  \n",
       "```python\n",
       "from sklearn.impute import SimpleImputer\n",
       "from sklearn.preprocessing import MinMaxScaler\n",
       "\n",
       "# Impute missing values and scale features\n",
       "def transform_data(dataset: pd.DataFrame):\n",
       "    features = dataset.drop(columns=['NObeyesdad', 'id'])\n",
       "    target = dataset['NObeyesdad'].values if 'NObeyesdad' in dataset.columns else None\n",
       "\n",
       "    numeric_imputer = SimpleImputer(strategy='mean')\n",
       "    features = numeric_imputer.fit_transform(features)\n",
       "\n",
       "    scaler = MinMaxScaler()\n",
       "    features = scaler.fit_transform(features)\n",
       "\n",
       "    return features, target\n",
       "```  \n",
       "\n",
       "2. **–û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ –∏ AutoML**  \n",
       "   –ê–≤—Ç–æ–º–∞—Ç–∏–∑–∏—Ä–æ–≤–∞–ª–∏ –ø—Ä–æ—Ü–µ—Å—Å –æ–±—É—á–µ–Ω–∏—è –º–æ–¥–µ–ª–∏ —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º —Ñ—Ä–µ–π–º–≤–æ—Ä–∫–∞ **FEDOT AutoML** –¥–ª—è —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ–≥–æ –∏—Å—Å–ª–µ–¥–æ–≤–∞–Ω–∏—è –≤—ã—Å–æ–∫–æ–∫–∞—á–µ—Å—Ç–≤–µ–Ω–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—ã—Ö –º–æ–¥–µ–ª–µ–π.  \n",
       "```python\n",
       "from fedot.api.main import Fedot\n",
       "from fedot.core.repository.tasks import Task, TaskTypesEnum\n",
       "\n",
       "def train_model(features, target):\n",
       "    # Configure and train the model\n",
       "    pipeline = Fedot(problem='classification', preset='best_quality', timeout=1.0, metric='accuracy')\n",
       "    pipeline.fit(features, target)\n",
       "    return pipeline\n",
       "```  \n",
       "\n",
       "3. **–û—Ü–µ–Ω–∫–∞ –∏ –ü—Ä–æ–≥–Ω–æ–∑—ã**  \n",
       "   –û—Ü–µ–Ω–∏–ª–∏ –ø—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –∏ –ø–æ–¥–≥–æ—Ç–æ–≤–∏–ª–∏ —Ñ–∞–π–ª—ã –¥–ª—è –æ—Ç–ø—Ä–∞–≤–∫–∏.  \n",
       "```python\n",
       "def create_submission():\n",
       "    predictions = model.predict(test_features)\n",
       "    submission_df = pd.DataFrame({'id': test_data['id'], 'NObeyesdad': predictions})\n",
       "    submission_df.to_csv(\"submission.csv\", index=False)\n",
       "```  \n",
       "\n",
       "## üìä –ú–µ—Ç—Ä–∏–∫–∏  \n",
       "\n",
       "–ü—Ä–æ–∏–∑–≤–æ–¥–∏—Ç–µ–ª—å–Ω–æ—Å—Ç—å –º–æ–¥–µ–ª–∏ –æ—Ü–µ–Ω–∏–≤–∞–µ—Ç—Å—è —Å –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ–º **—Ç–æ—á–Ω–æ—Å—Ç–∏ (accuracy)**, –∫–æ—Ç–æ—Ä–∞—è –∏–∑–º–µ—Ä—è–µ—Ç –¥–æ–ª—é –ø—Ä–∞–≤–∏–ª—å–Ω—ã—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π —Å—Ä–µ–¥–∏ –æ–±—â–µ–≥–æ —á–∏—Å–ª–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π:  \n",
       "- –¢–æ—á–Ω–æ—Å—Ç—å: 90.7%  \n",
       "–≠—Ç–æ –æ–∑–Ω–∞—á–∞–µ—Ç, —á—Ç–æ –º–æ–¥–µ–ª—å –ø—Ä–∞–≤–∏–ª—å–Ω–æ –æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–∞—Ç–µ–≥–æ—Ä–∏—é —Ä–∏—Å–∫–∞ –æ–∂–∏—Ä–µ–Ω–∏—è –¥–ª—è 9 –∏–∑ –∫–∞–∂–¥—ã—Ö 10 —á–µ–ª–æ–≤–µ–∫ –≤ –æ—Ü–µ–Ω–æ—á–Ω–æ–º –Ω–∞–±–æ—Ä–µ.  \n",
       "\n",
       "### –ò–Ω—Ç–µ—Ä–ø—Ä–µ—Ç–∞—Ü–∏—è:  \n",
       "- **–°–∏–ª—å–Ω—ã–µ —Å—Ç–æ—Ä–æ–Ω—ã:** –í—ã—Å–æ–∫–∞—è —Ç–æ—á–Ω–æ—Å—Ç—å –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç —Å–∏–ª—å–Ω—É—é –ø—Ä–µ–¥—Å–∫–∞–∑–∞—Ç–µ–ª—å–Ω—É—é —Å–ø–æ—Å–æ–±–Ω–æ—Å—Ç—å –ø–æ –Ω–µ—Å–∫–æ–ª—å–∫–∏–º –∫–ª–∞—Å—Å–∞–º.  \n",
       "- **–ü–æ—Ç–µ–Ω—Ü–∏–∞–ª—å–Ω—ã–µ –ø—Ä–æ–±–ª–µ–º—ã:** –î–∏—Å–±–∞–ª–∞–Ω—Å –∫–ª–∞—Å—Å–æ–≤ (–Ω–µ–∫–æ—Ç–æ—Ä—ã–µ –∫–∞—Ç–µ–≥–æ—Ä–∏–∏ –æ–∂–∏—Ä–µ–Ω–∏—è –º–æ–≥—É—Ç –∏–º–µ—Ç—å –º–µ–Ω—å—à–µ —ç–∫–∑–µ–º–ø–ª—è—Ä–æ–≤) –º–æ–∂–µ—Ç –ø–æ–≤–ª–∏—è—Ç—å –Ω–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è –º–∏–Ω–æ—Ä–∏—Ç–∞—Ä–Ω—ã—Ö –∫–ª–∞—Å—Å–æ–≤.  \n",
       "\n",
       "## üí° –í—ã–≤–æ–¥—ã  \n",
       "\n",
       "–≠—Ç–∞ –º–æ–¥–µ–ª—å —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä—É–µ—Ç —Ä–∏—Å–∫ –æ–∂–∏—Ä–µ–Ω–∏—è —Å **—Ç–æ—á–Ω–æ—Å—Ç—å—é 90.7%**, —á—Ç–æ —è–≤–ª—è–µ—Ç—Å—è –º–Ω–æ–≥–æ–æ–±–µ—â–∞—é—â–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–º –¥–ª—è –ø—Ä–∏–º–µ–Ω–µ–Ω–∏–π –≤ –æ–±–ª–∞—Å—Ç–∏ –æ–±—â–µ—Å—Ç–≤–µ–Ω–Ω–æ–≥–æ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è. –ò—Å–ø–æ–ª—å–∑—É—è –ø–µ—Ä–µ–¥–æ–≤—ã–µ –º–µ—Ç–æ–¥—ã AutoML –∏ –Ω–∞–¥–µ–∂–Ω—É—é –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω—É—é –æ–±—Ä–∞–±–æ—Ç–∫—É, –æ–Ω–∞ –¥–µ–º–æ–Ω—Å—Ç—Ä–∏—Ä—É–µ—Ç –º–∞—Å—à—Ç–∞–±–∏—Ä—É–µ–º—ã–π –∏ —ç—Ñ—Ñ–µ–∫—Ç–∏–≤–Ω—ã–π –ø–æ–¥—Ö–æ–¥ –∫ —Ä–µ—à–µ–Ω–∏—é –∞–Ω–∞–ª–æ–≥–∏—á–Ω—ã—Ö –∑–∞–¥–∞—á –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ü–∏–∏. –•–æ—Ç—è —ç—Ç–æ —Å–∏–Ω—Ç–µ—Ç–∏—á–µ—Å–∫–∏–π –Ω–∞–±–æ—Ä –¥–∞–Ω–Ω—ã—Ö –¥–ª—è –∫–æ–Ω–∫—É—Ä—Å–∞, –∫–æ–Ω–≤–µ–π–µ—Ä –º–æ–∂–µ—Ç –±—ã—Ç—å –ª–µ–≥–∫–æ –∞–¥–∞–ø—Ç–∏—Ä–æ–≤–∞–Ω –¥–ª—è —Ä–µ–∞–ª—å–Ω—ã—Ö —Å–ª—É—á–∞–µ–≤ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏—è, —Ç–∞–∫–∏—Ö –∫–∞–∫ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ —Å–µ—Ä–¥–µ—á–Ω–æ-—Å–æ—Å—É–¥–∏—Å—Ç–æ–≥–æ —Ä–∏—Å–∫–∞ –∏–ª–∏ —Ü–µ–ª–µ–Ω–∞–ø—Ä–∞–≤–ª–µ–Ω–Ω–æ–µ –¥–∏–µ—Ç–∏—á–µ—Å–∫–æ–µ –≤–º–µ—à–∞—Ç–µ–ª—å—Å—Ç–≤–æ.  \n",
       "\n",
       "–ú–æ–¥–µ–ª–∏—Ä–æ–≤–∞–Ω–∏–µ —Ä–∏—Å–∫–∞ –æ–∂–∏—Ä–µ–Ω–∏—è ‚Äî —ç—Ç–æ –Ω–µ —Ç–æ–ª—å–∫–æ –ø—Ä–æ–≥–Ω–æ–∑–∏—Ä–æ–≤–∞–Ω–∏–µ, —ç—Ç–æ –æ–±–µ—Å–ø–µ—á–µ–Ω–∏–µ –ø—Ä–µ–≤–µ–Ω—Ç–∏–≤–Ω—ã—Ö –º–µ—Ä –≤ –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏–∏, –∫–æ—Ç–æ—Ä—ã–µ –º–æ–≥—É—Ç —Å–ø–∞—Å—Ç–∏ –∂–∏–∑–Ω–∏. –≠—Ç–æ—Ç –∫–æ–Ω–∫—É—Ä—Å –ø–æ–∫–∞–∑—ã–≤–∞–µ—Ç, –∫–∞–∫ –º–∞—à–∏–Ω–Ω–æ–µ –æ–±—É—á–µ–Ω–∏–µ –º–æ–∂–µ—Ç –∑–Ω–∞—á–∏—Ç–µ–ª—å–Ω–æ –ø—Ä–æ–¥–≤–∏–Ω—É—Ç—å—Å—è –≤ —Ä–µ—à–µ–Ω–∏–∏ –≥–ª–æ–±–∞–ª—å–Ω—ã—Ö –ø—Ä–æ–±–ª–µ–º –∑–¥—Ä–∞–≤–æ–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import Markdown\n",
    "result = translator._translate_text(report, target_language=\"ru\")\n",
    "Markdown(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99bbd29d",
   "metadata": {},
   "source": [
    "## Example 1: Simple Text Translation\n",
    "\n",
    "Let's start with a basic translation from Spanish to English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8605e36a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (Spanish): Hola, ¬øc√≥mo est√°s? Me llamo Ana y soy ingeniera de software.\n",
      "Detected language: es. Translating to English using AIInference with improved formatting preservation.\n",
      "\n",
      "Translated to English: Hello, how are you? My name is Ana and I am a software engineer.\n",
      "Translating back to es using AIInference with improved formatting preservation.\n",
      "\n",
      "Translated to English: Hello, how are you? My name is Ana and I am a software engineer.\n",
      "Translating back to es using AIInference with improved formatting preservation.\n",
      "\n",
      "Back to Spanish: ¬øHola, c√≥mo est√°s? Me llamo Ana y soy ingeniera de software.\n",
      "\n",
      "Back to Spanish: ¬øHola, c√≥mo est√°s? Me llamo Ana y soy ingeniera de software.\n"
     ]
    }
   ],
   "source": [
    "# Spanish text\n",
    "spanish_text = \"Hola, ¬øc√≥mo est√°s? Me llamo Ana y soy ingeniera de software.\"\n",
    "\n",
    "print(f\"Original (Spanish): {spanish_text}\")\n",
    "\n",
    "# Translate to English\n",
    "english_translation = translator.translate_input_to_english(spanish_text)\n",
    "print(f\"\\nTranslated to English: {english_translation}\")\n",
    "\n",
    "# Translate back to original language\n",
    "back_translation = translator.translate_output_to_source_language(english_translation)\n",
    "print(f\"\\nBack to Spanish: {back_translation}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f373e1ea",
   "metadata": {},
   "source": [
    "## Example 2: Markdown Text with Code Blocks\n",
    "\n",
    "Now let's test with formatted text that includes markdown and code blocks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d444e986",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original (French with markdown):\n",
      "# Guide de Programmation\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Voici un exemple de **fonction Python** simple:\n",
      "\n",
      "```python\n",
      "def saluer(nom):\n",
      "    return f\"Bonjour {nom}!\"\n",
      "\n",
      "# Utilisation\n",
      "message = saluer(\"Marie\")\n",
      "print(message)\n",
      "```\n",
      "\n",
      "Cette fonction:\n",
      "- Prend un *param√®tre* nom\n",
      "- Retourne un message de salutation\n",
      "- Utilise les f-strings pour le formatage\n",
      "\n",
      "> **Note**: Les f-strings sont disponibles depuis Python 3.6\n",
      "\n",
      "\n",
      "==================================================\n",
      "Detected language: fr. Translating to English using AIInference with improved formatting preservation.\n",
      "\n",
      "Translated to English:\n",
      "# Programming Guide\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Here is an example of a **simple Python function**:\n",
      "\n",
      "```python\n",
      "def saluer(nom):\n",
      "    return f\"Bonjour {nom}!\"\n",
      "\n",
      "# Utilisation\n",
      "message = saluer(\"Marie\")\n",
      "print(message)\n",
      "```\n",
      "\n",
      "This function:\n",
      "- Takes a *name* parameter\n",
      "- Returns a greeting message\n",
      "- Uses f-strings for formatting\n",
      "\n",
      "> **Note**: F-strings are available since Python 3.6\n",
      "\n",
      "Translated to English:\n",
      "# Programming Guide\n",
      "\n",
      "## Introduction\n",
      "\n",
      "Here is an example of a **simple Python function**:\n",
      "\n",
      "```python\n",
      "def saluer(nom):\n",
      "    return f\"Bonjour {nom}!\"\n",
      "\n",
      "# Utilisation\n",
      "message = saluer(\"Marie\")\n",
      "print(message)\n",
      "```\n",
      "\n",
      "This function:\n",
      "- Takes a *name* parameter\n",
      "- Returns a greeting message\n",
      "- Uses f-strings for formatting\n",
      "\n",
      "> **Note**: F-strings are available since Python 3.6\n"
     ]
    }
   ],
   "source": [
    "# Reset translator for new language detection\n",
    "translator = TranslatorAgent(inference)\n",
    "\n",
    "# French text with markdown and code\n",
    "french_markdown = \"\"\"# Guide de Programmation\n",
    "\n",
    "## Introduction\n",
    "\n",
    "Voici un exemple de **fonction Python** simple:\n",
    "\n",
    "```python\n",
    "def saluer(nom):\n",
    "    return f\"Bonjour {nom}!\"\n",
    "\n",
    "# Utilisation\n",
    "message = saluer(\"Marie\")\n",
    "print(message)\n",
    "```\n",
    "\n",
    "Cette fonction:\n",
    "- Prend un *param√®tre* nom\n",
    "- Retourne un message de salutation\n",
    "- Utilise les f-strings pour le formatage\n",
    "\n",
    "> **Note**: Les f-strings sont disponibles depuis Python 3.6\n",
    "\"\"\"\n",
    "\n",
    "print(\"Original (French with markdown):\")\n",
    "print(french_markdown)\n",
    "print(\"\\n\" + \"=\"*50)\n",
    "\n",
    "# Translate to English\n",
    "english_markdown = translator.translate_input_to_english(french_markdown)\n",
    "print(\"\\nTranslated to English:\")\n",
    "print(english_markdown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4da4d2e4",
   "metadata": {},
   "source": [
    "## Example 3: Interactive Translation\n",
    "\n",
    "Try your own text! Modify the cell below with any text you want to translate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd16d271",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Your text: Guten Tag! Wie geht es Ihnen heute? Ich hoffe, Sie haben einen sch√∂nen Tag.\n",
      "\n",
      "Detecting language and translating...\n",
      "Detected language: de. Translating to English using AIInference with improved formatting preservation.\n",
      "\n",
      "English translation: Good day! How are you today? I hope you have a nice day.\n",
      "Translating back to de using AIInference with improved formatting preservation.\n",
      "\n",
      "English translation: Good day! How are you today? I hope you have a nice day.\n",
      "Translating back to de using AIInference with improved formatting preservation.\n",
      "\n",
      "Back to original language: Guten Tag! Wie geht es Ihnen heute? Ich hoffe, Sie haben einen sch√∂nen Tag.\n",
      "\n",
      "Back to original language: Guten Tag! Wie geht es Ihnen heute? Ich hoffe, Sie haben einen sch√∂nen Tag.\n"
     ]
    }
   ],
   "source": [
    "# Reset translator for new session\n",
    "translator = TranslatorAgent(inference)\n",
    "\n",
    "# Enter your text here\n",
    "your_text = \"Guten Tag! Wie geht es Ihnen heute? Ich hoffe, Sie haben einen sch√∂nen Tag.\"\n",
    "\n",
    "print(f\"Your text: {your_text}\")\n",
    "print(\"\\nDetecting language and translating...\")\n",
    "\n",
    "# Translate to English\n",
    "result = translator.translate_input_to_english(your_text)\n",
    "print(f\"\\nEnglish translation: {result}\")\n",
    "\n",
    "# Translate back\n",
    "back_result = translator.translate_output_to_source_language(result)\n",
    "print(f\"\\nBack to original language: {back_result}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2568b507",
   "metadata": {},
   "source": [
    "## Example 4: Testing Different Languages\n",
    "\n",
    "Let's test with multiple languages to see how the automatic detection works."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "d1ae8aaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "Testing Italian:\n",
      "Original: Ciao! Come va? Spero che tu stia bene oggi.\n",
      "Detected language: it. Translating to English using AIInference with improved formatting preservation.\n",
      "English: Hi! How are you? I hope you're doing well today.\n",
      "Detected language: it\n",
      "\n",
      "==================================================\n",
      "Testing Portuguese:\n",
      "Original: Ol√°! Como voc√™ est√°? Espero que tenha um √≥timo dia.\n",
      "Detected language: pt. Translating to English using AIInference with improved formatting preservation.\n",
      "English: Hello! How are you? I hope you have a great day.\n",
      "Detected language: pt\n",
      "\n",
      "==================================================\n",
      "Testing Russian:\n",
      "Original: –ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? –ù–∞–¥–µ—é—Å—å, —É —Ç–µ–±—è –≤—Å–µ —Ö–æ—Ä–æ—à–æ.\n",
      "Detected language: ru. Translating to English using AIInference with improved formatting preservation.\n",
      "English: Hi! How are you? I hope you're doing well.\n",
      "Detected language: ru\n",
      "\n",
      "==================================================\n",
      "Testing Japanese:\n",
      "Original: „Åì„Çì„Å´„Å°„ÅØÔºÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºüËâØ„ÅÑ‰∏ÄÊó•„Çí„ÅäÈÅé„Åî„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ\n",
      "Detected language: ja. Translating to English using AIInference with improved formatting preservation.\n",
      "English: Hello! How are you? Have a good day.\n",
      "Detected language: ja\n",
      "\n",
      "==================================================\n",
      "Testing English:\n",
      "Original: Hello! How are you? I hope you have a great day.\n",
      "Detected language: English. No translation needed.\n",
      "English: Hello! How are you? I hope you have a great day.\n"
     ]
    }
   ],
   "source": [
    "# Test texts in different languages\n",
    "test_texts = {\n",
    "    \"Italian\": \"Ciao! Come va? Spero che tu stia bene oggi.\",\n",
    "    \"Portuguese\": \"Ol√°! Como voc√™ est√°? Espero que tenha um √≥timo dia.\",\n",
    "    \"Russian\": \"–ü—Ä–∏–≤–µ—Ç! –ö–∞–∫ –¥–µ–ª–∞? –ù–∞–¥–µ—é—Å—å, —É —Ç–µ–±—è –≤—Å–µ —Ö–æ—Ä–æ—à–æ.\",\n",
    "    \"Japanese\": \"„Åì„Çì„Å´„Å°„ÅØÔºÅÂÖÉÊ∞ó„Åß„Åô„ÅãÔºüËâØ„ÅÑ‰∏ÄÊó•„Çí„ÅäÈÅé„Åî„Åó„Åè„Å†„Åï„ÅÑ„ÄÇ\",\n",
    "    \"English\": \"Hello! How are you? I hope you have a great day.\"\n",
    "}\n",
    "\n",
    "for language, text in test_texts.items():\n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"Testing {language}:\")\n",
    "    print(f\"Original: {text}\")\n",
    "    \n",
    "    # Reset translator for each language\n",
    "    translator = TranslatorAgent(inference)\n",
    "    \n",
    "    # Translate to English\n",
    "    english_result = translator.translate_input_to_english(text)\n",
    "    print(f\"English: {english_result}\")\n",
    "    \n",
    "    if translator.source_language != \"en\":\n",
    "        print(f\"Detected language: {translator.source_language}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25f65c6e",
   "metadata": {},
   "source": [
    "## Key Features Demonstrated\n",
    "\n",
    "This notebook shows the following features of TranslatorAgent:\n",
    "\n",
    "1. **Automatic Language Detection**: Automatically detects the source language\n",
    "2. **Bidirectional Translation**: Translate to English and back to original language\n",
    "3. **Markdown Preservation**: Maintains all markdown formatting\n",
    "4. **Code Block Protection**: Code blocks remain unchanged during translation\n",
    "5. **Multiple Language Support**: Works with many different languages\n",
    "6. **Robust Error Handling**: Gracefully handles translation failures\n",
    "\n",
    "## Notes\n",
    "\n",
    "- Make sure to set the `FEDOTLLM_LLM_API_KEY` environment variable\n",
    "- You can use different AI models by changing the `model` parameter\n",
    "- The translator preserves the source language for consistent back-translation\n",
    "- Code blocks and technical content remain unchanged during translation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084771aa",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
